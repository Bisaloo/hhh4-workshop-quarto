[
  {
    "objectID": "tutorials/introduction.html",
    "href": "tutorials/introduction.html",
    "title": "Tutorial 1: “sts” class and univariate hhh4()",
    "section": "",
    "text": "This tutorial illustrates how to import time series data as an \"sts\" (surveillance time series) object as used by the R package surveillance. As an example, we will import a (gzip-compressed) csv file downloaded from the ECDC Surveillance Atlas of Infectious Diseases. It contains the monthly number of reported cases of invasive meningococcal disease (IMD).\nThe code to import the raw data is already provided here and in the R script template so that you don’t lose time with this step and can concentrate on the sts() and hhh4() exercises further below.\nBefore you start, here is what Wikipedia says about the epidemiology of meningococcus, the bacterium causing IMD:"
  },
  {
    "objectID": "tutorials/introduction.html#import-count-time-series",
    "href": "tutorials/introduction.html#import-count-time-series",
    "title": "Tutorial 1: “sts” class and univariate hhh4()",
    "section": "Import count time series",
    "text": "Import count time series\nThis is how the first 3 lines of the csv file from ECDC look like:\n\n\n\"HealthTopic\",\"Population\",\"Indicator\",\"Unit\",\"Time\",\"RegionCode\",\"RegionName\",\"NumValue\",\"TxtValue\"\n\"Invasive meningococcal disease\",\"Confirmed cases\",\"Reported cases\",\"N\",\"1999-01\",\"AT\",\"Austria\",11.000000000,\"\"\n\"Invasive meningococcal disease\",\"Confirmed cases\",\"Reported cases\",\"N\",\"1999-01\",\"BE\",\"Belgium\",24.000000000,\"\"\n\n\n\n\nDisclaimer (ECDC)\n\nThe views and opinions of the authors expressed herein do not necessarily state or reflect those of the ECDC. The accuracy of the authors’ statistical analysis and the findings they report are not the responsibility of ECDC. ECDC is not responsible for conclusions or opinions drawn from the data provided. ECDC is not responsible for the correctness of the data and for data management, data merging and data collation after provision of the data. ECDC shall not be held liable for improper or incorrect use of the data.\n\nThe first step is to import the counts from the csv file and reshape the data into the wide (matrix) format typically used for multivariate time series and expected by sts() and basic ts().\n\necdc_long <- read.csv(\"data/ECDC_surveillance_data_IMD.csv.gz\",\n                      na.strings = \"-\") # always important to know NA encoding!\n## exclude aggregate counts\necdc_long <- subset(ecdc_long, !RegionName %in% c(\"EU/EEA\", \"EU\"))\n\n## reshape from long to wide format of multivariate time series\necdc <- reshape(ecdc_long[c(\"Time\", \"RegionCode\", \"NumValue\")],\n                direction = \"wide\", idvar = \"Time\", timevar = \"RegionCode\")\nnames(ecdc) <- sub(\"NumValue.\", \"\", names(ecdc), fixed = TRUE)\nrow.names(ecdc) <- ecdc$Time; ecdc$Time <- NULL\n\nhead(ecdc)\n\n        AT BE CZ DE DK EE EL  ES FI FR IE IS IT LU MT NL NO PL SE SI  UK LT PT LV HU SK CY BG RO HR\n1999-01 11 24  5 37 19  0 18 172  6 43 53  2 36  0  0 81  4 11  2  1 495 NA NA NA NA NA NA NA NA NA\n1999-02 14 37  8 48 17  0 16 123  9 65 43  2 31  0  2 68 11  9  3  0 267 NA NA NA NA NA NA NA NA NA\n1999-03 12 49 11 54 28  0 42  90  5 35 53  1 29  0  3 82 10  8  6  0 224 NA NA NA NA NA NA NA NA NA\n1999-04  6 23  9 36 22  1 21  70  3 30 40  2 23  0  0 33  4  4  2  1 203 NA NA NA NA NA NA NA NA NA\n1999-05  3 27 14 36 11  0 16  78  7 18 34  2 18  0  1 41  6  6  3  1 140 NA NA NA NA NA NA NA NA NA\n1999-06  4 25 10 30 12  0 11  50  8 19 24  2 21  0  1 38  4  1  2  0 199 NA NA NA NA NA NA NA NA NA\n\ntail(ecdc)\n\n        AT BE CZ DE DK EE EL ES FI FR IE IS IT LU MT NL NO PL SE SI UK LT PT LV HU SK CY BG RO HR\n2018-07  1 10  8 18  5  1  2 25  1 34  5  0 12 NA  0 16  2  7  6  1 28  1  5  0  0  2  0 NA  3 NA\n2018-08  4  1  7 15  1  0  3 13  1 17  5  0  8 NA  1 13  1  8  2  0 32  1  4  0  0  2  0 NA  6 NA\n2018-09  1  5  0 14  0  0  1 22  2 17  7  0  7 NA  0  7  1  4  6  2 44  2  1  1  4  0  0 NA  4 NA\n2018-10  2 13  3 27  4  0  2 32  3 30  4  0 12 NA  0 21  3 18  6  2 57  1  3  1  2  5  0 NA  6 NA\n2018-11  3  6  7 15  3  0  4 26  0 28  4  0 10 NA  0 10  0 21  1  2 44  1  5  0  3  1  0 NA  2 NA\n2018-12  1  9  7 17  3  0  4 34  1 47  6  0 10 NA  0 13  2 21  7  3 60  3  5  1  6  2  0 NA  4 NA\n\n## exclude contries without data\necdc <- ecdc[, colSums(!is.na(ecdc)) > 0]\n\nCountry codes are given in the Eurostat Glossary."
  },
  {
    "objectID": "tutorials/introduction.html#import-population-data",
    "href": "tutorials/introduction.html#import-population-data",
    "title": "Tutorial 1: “sts” class and univariate hhh4()",
    "section": "Import population data",
    "text": "Import population data\nPopulation counts are needed to calculate incidence values. Eurostat provides such data as table “TPS00001”; we have downloaded the values for 2018 as a csv file.\n\npopdata <- read.csv(\"data/Eurostat_population_2018.csv\")\nhead(popdata, 3)\n\n             DATAFLOW       LAST.UPDATE freq indic_de geo TIME_PERIOD OBS_VALUE OBS_FLAG\n1 ESTAT:TPS00001(1.0) 17/03/22 23:00:00    A      JAN  AD        2018     74794        e\n2 ESTAT:TPS00001(1.0) 17/03/22 23:00:00    A      JAN  AL        2018   2870324         \n3 ESTAT:TPS00001(1.0) 17/03/22 23:00:00    A      JAN  AM        2018   2972732         \n\nstopifnot(colnames(ecdc) %in% popdata$geo)\npop <- setNames(popdata$OBS_VALUE, popdata$geo)[colnames(ecdc)]\npop\n\n      AT       BE       CZ       DE       DK       EE       EL       ES       FI       FR       IE \n 8822267 11398589 10610055 82792351  5781190  1319133 10741165 46658447  5513130 67026224  4830392 \n      IS       IT       LU       MT       NL       NO       PL       SE       SI       UK       LT \n  348450 60483973   602005   475701 17181084  5295619 37976687 10120242  2066880 66273576  2808901 \n      PT       LV       HU       SK       CY       RO \n10291027  1934379  9778371  5443120   864236 19533481"
  },
  {
    "objectID": "tutorials/introduction.html#import-map",
    "href": "tutorials/introduction.html#import-map",
    "title": "Tutorial 1: “sts” class and univariate hhh4()",
    "section": "Import map",
    "text": "Import map\nThe \"sts\" class can be used without a supplementary map, but incorporating one enables a spatial view of the data. We retrieve a suitable GeoJSON dataset of administrative boundaries from Eurostat/GISCO, with Copyright (C) EuroGeographics. The result of importing that dataset is available as map.RData (so as to avoid potential problems with system requirements for sf).\n\nfile_map <- \"data/map.RData\"\nif (file.exists(file_map)) {\n   load(file_map)\n} else {\n  library(\"sf\")\n  ## read NUTS1-level data from Eurostat/GISCO\n  map1 <- st_read(\"https://gisco-services.ec.europa.eu/distribution/v2/nuts/geojson/NUTS_RG_60M_2021_4326_LEVL_1.geojson\")\n  ## omit French overseas regions for a more compact map\n  map1 <- subset(map1, NUTS_ID != \"FRY\")\n  ## union polygons by country\n  map0 <- aggregate(map1[0], by = list(COUNTRY = map1$CNTR_CODE), FUN = sum)\n  ## check that the map contains the country codes of colnames(ecdc)\n  stopifnot(colnames(ecdc) %in% map0$COUNTRY)\n  ## convert to \"SpatialPolgons\" for use with sts()\n  row.names(map0) <- map0$COUNTRY  # to match with colnames(ecdc)\n  library(\"sp\")\n  map <- geometry(as_Spatial(map0))\n  save(map, file = file_map, compress = \"xz\")\n}\n\n\nlibrary(\"sp\")\npar(mar = c(0,0,0,0))\nplot(map)"
  },
  {
    "objectID": "tutorials/introduction.html#create-an-sts-object",
    "href": "tutorials/introduction.html#create-an-sts-object",
    "title": "Tutorial 1: “sts” class and univariate hhh4()",
    "section": "Create an “sts” object",
    "text": "Create an “sts” object\nUsing the above ingredients (ecdc, pop, map), we can now create an \"sts\" object via sts().\n\nlibrary(\"surveillance\")\n\n## start of the monthly time series\n(start <- as.numeric(strsplit(rownames(ecdc)[1], split=\"-\")[[1]]))\n\n[1] 1999    1\n\n## IMD0 <- sts(....)  # see help(\"sts\")\n\n\n\nsolution\n\n\nIMD0 <- sts(ecdc, start = start, frequency = 12, # monthly data\n            population = pop, map = map)\n\n\n\nPlot the (aggregated) count time series. Which problem do you notice?\n\n\nsolution\n\n\nplot(IMD0, type = observed ~ time)\n\n\n\n\n\n\n\n## the time series obviously contains missing values\n\n## ggplot2 version:\nautoplot.sts(aggregate(IMD0, by = \"unit\"))\n\nWarning: Removed 227 rows containing missing values (position_stack).\n\n\n\n\n\n\n\n\n\n\n\nTo keep things simple, we restrict the remaining exercises to a subset of the countries:\n\nIMD <- IMD0[, grep(\"^[ADFNU]\", colnames(IMD0))]"
  },
  {
    "objectID": "tutorials/introduction.html#visualizations",
    "href": "tutorials/introduction.html#visualizations",
    "title": "Tutorial 1: “sts” class and univariate hhh4()",
    "section": "Visualizations",
    "text": "Visualizations\n\nobserved ~ time\nPlot the overall and country-specific count time series, using either the conventional plot method or autoplot.sts() (requires ggplot2). The latter has an argument to plot incidences instead of counts.\n\n\nsolution\n\n\n## count time series\nplot(IMD, type = observed ~ time)  # overall\n\n\n\n\n\n\n\nplot(IMD)\n\n\n\n\n\n\n\nautoplot.sts(IMD)\n\n\n\n\n\n\n\n## incidence time series (per 100'000 inhabitants)\nautoplot.sts(IMD, population = 100000)\n\n\n\n\n\n\n\n\n\n\n\n\nobserved ~ unit\nProduce a map with the country-specific cumulative number of cases (or incidence) over time. See help(\"stsplot_space\") for options.\n\n\nsolution\n\n\nplot(IMD, type = observed ~ unit)\n\n\n\n\n\n\n\n## cumulative incidence (per 100'000 inhabitants),\n## using some of the graphical options\nplot(IMD, type = observed ~ unit, population = 100000,\n     col.regions = hcl.colors(100, rev = TRUE),\n     gpar.missing = list(lty = 1, col = 8),\n     col = \"white\", lwd = 2,\n     sub = \"cumulative incidence (per 100'000 inhabitants)\")\n\n\n\n\n\n\n\n\n\n\n\n\nPlot via conversion to other data classes\n\"sts\" objects can be converted to\n\nthe basic \"ts\" class via as.ts(),\nthe xts format via as.xts.sts(),\na “tidy” (long) data.frame via tidy.sts() that can be used as input for, e.g., ggplot2 or lattice.\n\nTry to produce some alternative plots of the time series.\n\n\nsolution\n\n\n## basic \"ts\"\nplot(as.ts(IMD), las = 1, type = \"h\")  # note the varying y-axes\n\n\n\n\n\n\n\n## \"xts\"\nlibrary(\"xts\")\nplot(as.xts(IMD), multi.panel = TRUE, yaxis.same = FALSE)\n\n\n\n\n\n\n\n## \"lattice\"\nstr(IMDDF <- tidy.sts(IMD))\n\n'data.frame':   1920 obs. of  12 variables:\n $ epoch        : int  1 2 3 4 5 6 7 8 9 10 ...\n $ unit         : Factor w/ 8 levels \"AT\",\"DE\",\"DK\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year         : num  1999 1999 1999 1999 1999 ...\n $ freq         : num  12 12 12 12 12 12 12 12 12 12 ...\n $ epochInYear  : num  1 2 3 4 5 6 7 8 9 10 ...\n $ epochInPeriod: num  0.0833 0.1667 0.25 0.3333 0.4167 ...\n $ date         : Date, format: \"1999-01-01\" \"1999-02-01\" \"1999-03-01\" ...\n $ observed     : num  11 14 12 6 3 4 4 7 9 3 ...\n $ state        : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ alarm        : logi  NA NA NA NA NA NA ...\n $ upperbound   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ population   : int  8822267 8822267 8822267 8822267 8822267 8822267 8822267 8822267 8822267 8822267 ...\n\nlibrary(\"lattice\")\nxyplot(observed ~ date | unit, data = IMDDF, type = \"h\", as.table = TRUE)\n\n\n\n\n\n\n\n## \"ggplot2\"\nlibrary(\"ggplot2\")\nggplot(IMDDF, aes(x = date, y = observed)) + geom_col() + facet_wrap(~ unit)"
  },
  {
    "objectID": "tutorials/introduction.html#univariate-hhh4-modelling-exercise",
    "href": "tutorials/introduction.html#univariate-hhh4-modelling-exercise",
    "title": "Tutorial 1: “sts” class and univariate hhh4()",
    "section": "Univariate hhh4() modelling exercise",
    "text": "Univariate hhh4() modelling exercise\n\nIntroduction\nWe will now estimate simple, univariate hhh4() models for the IMD time series from France:\n\nIMD1 <- IMD[,\"FR\"]\nplot(IMD1)\n\n\n\n\n\n\n\n## plot yearly time series to investigate seasonality, here using \"lattice\"\nlibrary(\"lattice\")\nxyplot(observed ~ factor(epochInYear), data = tidy.sts(IMD1),\n       groups = year, type = \"l\", xlab = \"month\")\n\n\n\n\n\n\n\n\nSeasonality can be captured by including transformations of time (sine-cosine terms) as covariates in the model.\nWith monthly data, we will miss some of the infection dynamics and essentially aggregate over two generations. On the other hand, including a single lag as in the original formulation of the observation-driven hhh4 model seems sufficient.\nIn statistical terms, we will fit simple models of the form\n\\[\nY_t | Y_{t-1} \\sim \\operatorname{NegBin}(\\lambda_t y_{t-1} + \\nu_t, \\phi)\n\\]\nwhere the log-linear predictors \\(\\lambda_t\\) (ar) and \\(\\nu_t\\) (end) could both use the regression formula\n\naddSeason2formula(~ t, S = s, period = 12)\n\nwith s being the number of harmonics (each harmonic needing two parameters, amplitude and shift). In mathematical notation, this amounts to log-linear predictors of the form\n\\[\n\\alpha + \\beta t +\n\\sum_{s=1}^S \\left\\{ \\gamma_s \\sin(s \\omega t) +\n                     \\delta_s \\cos(s \\omega t) \\right\\}\n\\]\nwith fundamental frequency \\(\\omega=2\\pi/12\\).\n\n\nTasks\n\nEstimate a few hhh4() models of the above form (with or without trend) and compare them using AIC() (Akaike Information Criterion). Note that you can easily estimate a new model with an updated number of harmonics and dropped AR trend from a previous model fit:\n\n\nfit2 <- update(fit1, ar = list(f = ~ 1), S = list(ar = 2, end = 2))\n\n\nPlot the fitted time series and the estimated seasonal curves of one or more useful models.\nUse oneStepAhead() to run rolling monthly forecasts over the last two years and plot() the result. You could also look at a pit() histogram of these probabilistic forecasts.\n\n\n\nsolution\n\n\n## model 1 with time trend and single sinusoidal curve in both components\n(form1 <- addSeason2formula(~t, S = 1, period = 12))\n\n~t + sin(2 * pi * t/12) + cos(2 * pi * t/12)\n\nfit1 <- hhh4(IMD1, control = list(ar = list(f = form1),\n                                  end = list(f = form1),\n                                  family = \"NegBin1\"))\nsummary(fit1)\n\n\nCall: \nhhh4(stsObj = IMD1, control = list(ar = list(f = form1), end = list(f = form1), \n    family = \"NegBin1\"))\n\nCoefficients:\n                        Estimate    Std. Error\nar.1                    -0.9109893   0.2132560\nar.t                    -0.0008338   0.0010388\nar.sin(2 * pi * t/12)    0.0150849   0.1525758\nar.cos(2 * pi * t/12)   -0.2375763   0.1751476\nend.1                    3.3798524   0.1314850\nend.t                   -0.0003699   0.0005889\nend.sin(2 * pi * t/12)   0.0558467   0.0890148\nend.cos(2 * pi * t/12)   0.4992783   0.1029933\noverdisp                 0.0304332   0.0047301\n\nLog-likelihood:   -895.61 \nAIC:              1809.22 \nBIC:              1840.5 \n\nNumber of units:        1 \nNumber of time points:  239 \n\n## for parameter interpretation:\nsummary(fit1, maxEV = TRUE, amplitudeShift = TRUE, idx2Exp = TRUE)\n\n\nCall: \nhhh4(stsObj = IMD1, control = list(ar = list(f = form1), end = list(f = form1), \n    family = \"NegBin1\"))\n\nCoefficients:\n                      Estimate    Std. Error\nexp(ar.1)              0.4021262   0.0857558\nexp(ar.t)              0.9991665   0.0010379\nar.A(2 * pi * t/12)    0.2380547   0.1787364\nar.s(2 * pi * t/12)   -1.5073863   0.0972142\nexp(end.1)            29.3664369   3.8612452\nexp(end.t)             0.9996302   0.0005887\nend.A(2 * pi * t/12)   0.5023919   0.0978861\nend.s(2 * pi * t/12)   1.4594044   0.0612924\noverdisp               0.0304332   0.0047301\n\nEpidemic dominant eigenvalue:  0.26 -- 0.51 \n\nLog-likelihood:   -895.61 \nAIC:              1809.22 \nBIC:              1840.5 \n\nNumber of units:        1 \nNumber of time points:  239 \n\n## model 2 without time trends\nform2 <- addSeason2formula(~1, S = 1, period = 12)\nfit2 <- hhh4(IMD1, control = list(ar = list(f = form2),\n                                  end = list(f = form2),\n                                  family = \"NegBin1\"))\nsummary(fit2, maxEV = TRUE, amplitudeShift = TRUE, idx2Exp = TRUE)\n\n\nCall: \nhhh4(stsObj = IMD1, control = list(ar = list(f = form2), end = list(f = form2), \n    family = \"NegBin1\"))\n\nCoefficients:\n                      Estimate   Std. Error\nexp(ar.1)              0.399107   0.064866 \nar.A(2 * pi * t/12)    0.219542   0.153981 \nar.s(2 * pi * t/12)   -1.524598   0.091334 \nexp(end.1)            26.530592   2.917682 \nend.A(2 * pi * t/12)   0.521711   0.101092 \nend.s(2 * pi * t/12)   1.475408   0.063261 \noverdisp               0.031759   0.004843 \n\nEpidemic dominant eigenvalue:  0.32 -- 0.50 \n\nLog-likelihood:   -898.38 \nAIC:              1810.75 \nBIC:              1835.09 \n\nNumber of units:        1 \nNumber of time points:  239 \n\nAIC(fit1, fit2)\n\n     df      AIC\nfit1  9 1809.216\nfit2  7 1810.753\n\n## model 3 with two harmonics\nfit3 <- update(fit2, S = list(ar = 2, end = 2))\nAIC(fit2, fit3)\n\n     df      AIC\nfit2  7 1810.753\nfit3 11 1802.036\n\n## fitted values and estimated seasonality\nplot(fit3)\n\n\n\n\n\n\n\nplot(fit3, type = \"season\")\n\n\n\n\n\n\n\n## compute rolling monthly forecasts\nosa <- oneStepAhead(fit3, nrow(IMD1) - 2*52, verbose = interactive())\nplot(osa)\n\n\n\n\n\n\n\nhead(quantile(osa))  # for prediction intervals\n\n    2.5 % 10.0 % 50.0 % 90.0 % 97.5 %\n137    24     29     39     52     59\n138    22     27     37     48     55\n139    25     30     41     54     61\n140    21     26     36     47     54\n141    19     23     33     43     49\n142    24     29     40     52     59\n\npit(osa)  # model tends to overpredict"
  },
  {
    "objectID": "tutorials/multivariate.html",
    "href": "tutorials/multivariate.html",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "",
    "text": "In this part of the tutorial you will learn how to build multivariate hhh4 models. We will proceed as follows:\n\nI will talk you through an example analysis of norovirus in the twelve districts of Berlin, adding complexity to an hhh4 model step by step.\nThen you will be able to use the code chunks from my example to analyse a data set on rotavirus in Berlin which has exactly the same structure as the norovirus data.\nWe’ll try to be available for questions while you are working on your analysis (as time and the number of participants permits).\nIf time permits we will go through some of your work at the end.\nWe are using these teaching materials for the first time, so they are likely to contain some errors and bugs – don’t hesitate to ask if something seems puzzling."
  },
  {
    "objectID": "tutorials/multivariate.html#repetition-installing-packages",
    "href": "tutorials/multivariate.html#repetition-installing-packages",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "Repetition: Installing packages",
    "text": "Repetition: Installing packages\nWe require the packages surveillance and hhh4addon, the latter containing some additional functionality for hhh4 models. It can be installed directly from GitHub using the remotes package (you may already have done that for the first part of the tutorial, but the necessary code is included here to make this document self-contained).\n\n# load packages:\n# options(warn = -1)\nlibrary(surveillance)\n\nLoading required package: sp\n\n\nLoading required package: xtable\n\n\nThis is surveillance 1.20.0. For overview type 'help(surveillance)'.\n\n# to install hhh4addon\n# install.packages(\"remotes\")\n# library(remotes)\n# remotes:::install_github(\"jbracher/hhh4addon\", build_vignettes = TRUE)\nlibrary(hhh4addon)\n\nLoading required package: polyCub\n\n\nThis is a development version of hhh4addon. It modifies and extends functionalities of surveillance:hhh4.\n\n\n\nAttaching package: 'hhh4addon'\n\n\nThe following object is masked from 'package:surveillance':\n\n    decompose.hhh4"
  },
  {
    "objectID": "tutorials/multivariate.html#example-data",
    "href": "tutorials/multivariate.html#example-data",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "Example data",
    "text": "Example data\nWe will use two data sets provided with the hhh4addon package (original data from Robert Koch Institute). The noroBE data contain weekly counts confirmed of norovirus cases in the twelve districts of Berlin, 2011–2017. The rotaBE data contain the same for rotavirus. In this note the norovirus data are analysed, which as an exercise you will be able to apply to the rotavirus data.\n\n# get the data\ndata(\"noroBE\")\n# if you don't have the hhh4addon package installed you can use:\n# load(\n#   url(\"https://github.com/cmmid/hhh4-workshop/blob/main/tutorial2_multivariate/data/noroBE.rda?raw=true\")\n# )\n\nWe start by plotting the temporal and spatial distribution of the data:\n\n# look at data:\n# time series:\nplot(noroBE, ylim = c(0, 40))\n\n\n\n# map:\n# as population(noroBE) contains population fractions rather than raw\n# population sizes setting population = 100,000/<population of Berlin>\n# will yield cumulative incidence per 100,000 inhabitants; see ?stsplot_space\nplot(noroBE, observed ~ unit, population = 100000/3500000, labels = TRUE)\n\n\n\n\nWe moreover take a look at the slot neighbourhood of the sts object. In this case it contains path distances between the districts: e.g., chwi and frkr have a distance of 2 as you need to go through mitt or scho.\n\n# check the neighbourhood structure, which is stored in the @neighbourhood slot \n# of the sts object:\nnoroBE@neighbourhood\n\n     chwi frkr lich mahe mitt neuk pank rein span zehl scho trko\nchwi    0    2    3    4    1    2    2    1    1    1    1    3\nfrkr    2    0    1    2    1    1    1    2    3    2    1    1\nlich    3    1    0    1    2    2    1    2    3    3    2    1\nmahe    4    2    1    0    3    2    2    3    4    4    3    1\nmitt    1    1    2    3    0    2    1    1    2    2    1    2\nneuk    2    1    2    2    2    0    2    3    3    2    1    1\npank    2    1    1    2    1    2    0    1    2    3    2    2\nrein    1    2    2    3    1    3    1    0    1    2    2    3\nspan    1    3    3    4    2    3    2    1    0    1    2    4\nzehl    1    2    3    4    2    2    3    2    1    0    1    3\nscho    1    1    2    3    1    1    2    2    2    1    0    2\ntrko    3    1    1    1    2    1    2    3    4    3    2    0"
  },
  {
    "objectID": "tutorials/multivariate.html#developing-a-multivariate-model-step-by-step",
    "href": "tutorials/multivariate.html#developing-a-multivariate-model-step-by-step",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "Developing a multivariate model step by step",
    "text": "Developing a multivariate model step by step\n\nStep 1: Seasonality in the endemic component, all parameters shared.\nWe start by formulating a very simple model where for districts \\(i = 1, \\dots, K\\) we assume \\[\\begin{align}\nY_{it} \\ \\mid \\ \\ \\text{past} & \\sim \\text{NegBin}(\\mu_{it}, \\psi)\\\\\n\\mu_{it} & = \\nu_t + \\lambda Y_{i, t - 1}\n\\end{align}\\] and\n\\[\n\\nu_{t} = \\alpha^\\nu + \\beta^\\nu \\sin(2\\pi t/52) + \\gamma^\\nu \\cos(2\\pi t/52).\n\\]\nReminder: we refer to\n\n\\(\\nu_t\\) as the endemic component\n\\(\\lambda Y_{i, t - 1}\\) as the autoregressive or epidemic component\n\nIn a setting with just three districts the model could be illustrated as follows – case numbers the different districts are independent and only depend on the previous value in the same district.\n\nIn surveillance this model is specified as follows. Reminder: The hhh4 function takes the following arguments:\n\nan stsobject containing the data\na control argument describing the model formulation. This list contains a number of named arguments, the most important ones being:\n\nend: a specification of the endemic component (i.e., \\(\\nu\\)).\nar: a specification of the autoregressive component (\\(\\lambda\\); dependence on the previous value from the same district).\nne: a specification of the neighbourhood component (\\(\\phi\\); dependence on on previous value from other districts, to be introduced later). All of end, ar and ne are lists containing a formula and possibly some more elements, see ?hhh4\nfamily: the type of distribution used in the recursive model formulation (either \"Poisson\", \"NegBin1\", or \"NegBinM\").\nsubset: The subset of the time series to which the model shall be fit.\n\n\n\n# first define a subset of the data to which to fit (will be used in all model\n# fits to ensure comparability of AIC values):\nsubset_fit <- 6:(nrow(noroBE@observed) - 52)\n# we are leaving out the last year and the first 5 observations\n# the latter serves to ensure comparability to later model versions\n\n##################################################################\n# Model 1:\n# control list:\nctrl1 <- list(end = list(f = addSeason2formula(~1, S = 1)), # seasonality in end\n              # S = 1 means one pair of sine / cosine waves is included\n              ar = list(f = ~ 1), # no seasonality in ar\n              family = \"NegBin1\", # negative binomial (rather than Poisson)\n              subset = subset_fit)\n# fit model\nfit1 <- hhh4(noroBE, ctrl1)\n# summary of parameter estimates:\nsummary(fit1)\n\n\nCall: \nhhh4(stsObj = noroBE, control = ctrl1)\n\nCoefficients:\n                        Estimate  Std. Error\nar.1                    -0.72854   0.03453  \nend.1                    0.84477   0.02860  \nend.sin(2 * pi * t/52)   0.09024   0.02806  \nend.cos(2 * pi * t/52)   0.88574   0.02856  \noverdisp                 0.22957   0.01076  \n\nLog-likelihood:   -8780.82 \nAIC:              17571.64 \nBIC:              17602.7 \n\nNumber of units:        12 \nNumber of time points:  307 \n\n# you can set idx2Exp = TRUE to get all estimates on the exp-transformed scale\nsummary(fit1, idx2Exp = TRUE)\n\n\nCall: \nhhh4(stsObj = noroBE, control = ctrl1)\n\nCoefficients:\n                             Estimate  Std. Error\nexp(ar.1)                    0.48261   0.01667   \nexp(end.1)                   2.32745   0.06657   \nexp(end.sin(2 * pi * t/52))  1.09443   0.03071   \nexp(end.cos(2 * pi * t/52))  2.42477   0.06925   \noverdisp                     0.22957   0.01076   \n\nLog-likelihood:   -8780.82 \nAIC:              17571.64 \nBIC:              17602.7 \n\nNumber of units:        12 \nNumber of time points:  307 \n\n# get AIC to compare model fit to more complex models\nAIC(fit1)\n\n[1] 17571.64\n\n# visually inspect:\npar(mfcol = c(6, 2))\nplot(fit1, unit = 1:12, par.settings = list(mfcol = c(6, 2))) # look at all units\n\n\n\n# par.settings = list(mfcol = c(6, 2)) tells the function we want two columns\n# of plots\n\nThe grey area represents the endemic component (\\(\\nu_{t}\\)) of the fitted values with clearly visible seasonality. The blue area represents the autoregressive component.\nFor model inspection I like looking at Pearson residuals\n\\[\n\\frac{Y_{it} - \\hat{\\mu}_{it}}{\\hat{\\sigma}_{it}} = \\frac{Y_{it} - \\hat{\\mu}_{it}}{\\sqrt{\\hat{\\mu}_{it} + \\hat{\\mu}_{it}^2/\\psi}},\n\\]\nwhich are not currently implemented in surveillance, but can be computed using a little auxiliary function. For now I just look at their mean and variance per district.\n\n# helper function to compute Pearson residuals:\npearson_residuals <- function(hhh4Obj){\n  # compute raw residuals:\n  response_residuals <- residuals(hhh4Obj, type = \"response\")\n  # compute standard deviations:\n  if (inherits(hhh4Obj, \"hhh4lag\")) {\n    sds <- sqrt(fitted(hhh4Obj) + \n                  fitted(hhh4Obj)^2/hhh4addon:::psi2size.hhh4lag(hhh4Obj))\n  } else {\n    sds <- sqrt(fitted(hhh4Obj) + \n                  fitted(hhh4Obj)^2/surveillance:::psi2size.hhh4(hhh4Obj))\n  }\n  # compute Pearson residuals:\n  pearson_residuals <- response_residuals/sds\n  return(pearson_residuals)\n}\n\npr1 <- pearson_residuals(fit1)\n# compute district-wise means and standard deviations:\ncolMeans(pr1)\n\n        chwi         frkr         lich         mahe         mitt         neuk \n-0.040128818 -0.354497174 -0.026596771 -0.149654100 -0.098957012 -0.139634691 \n        pank         rein         span         zehl         scho         trko \n 0.363677331 -0.006394507 -0.203281407  0.422776136  0.208763434 -0.023204469 \n\napply(pr1, 2, sd)\n\n     chwi      frkr      lich      mahe      mitt      neuk      pank      rein \n0.8969573 0.8394516 0.9984702 0.9348625 0.9594755 0.8998828 1.1386447 0.9955265 \n     span      zehl      scho      trko \n0.9606598 1.2538557 1.0589218 0.9730101 \n\n\nIn each district, the Pearson residuals should have mean and standard deviations of approximately 0 and 1, respectively, which is clearly not the case. By sharing all parameters, fitted values are systematically larger than observed in some districts and smaller in others.\n\n\nAdding district-specific parameters.\nAs the districts are of quite different sizes and have different levels of incidence, it seems reasonable to use district-specific parameters and extend the model to \\[\\begin{align}\nY_{it} \\ \\mid \\ \\ \\text{past} & \\sim \\text{NegBin}(\\mu_{it}, \\psi)\\\\\n\\mu_{it} & = \\nu_{it} + \\lambda_i Y_{i, t - 1}\\\\\n\\nu_{it} & = \\alpha^\\nu_i + \\beta^\\nu \\sin(2\\pi t/52) + \\gamma^\\nu \\cos(2\\pi t/52)\n\\end{align}\\]\n\n##################################################################\n# Model 2:\n# We use fe(..., unitSpecific = TRUE) to add fixed effects for each unit,\n# in this case intercepts (1). Seasonality parameters are still shared\n# across districts\nctrl2 <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              ar = list(f = ~ 0 + fe(1, unitSpecific = TRUE)),\n              family = \"NegBin1\",\n              subset = subset_fit)\n# Note: ~0 is necessary as otherwise one would (implicitly) add two intercepts\n# unit-specific dispersion parameters could be added by setting family = \"NegBinM\"\nfit2 <- hhh4(noroBE, ctrl2)\n# check parameter estimates:\nsummary(fit2)\n\n\nCall: \nhhh4(stsObj = noroBE, control = ctrl2)\n\nCoefficients:\n                        Estimate   Std. Error\nar.1.chwi               -1.795086   0.402460 \nar.1.frkr               -1.259748   0.182296 \nar.1.lich               -1.087080   0.177651 \nar.1.mahe               -0.957952   0.145501 \nar.1.mitt               -1.335034   0.214060 \nar.1.neuk               -0.967629   0.159855 \nar.1.pank               -0.753360   0.129017 \nar.1.rein               -0.668191   0.103620 \nar.1.span               -0.823339   0.125950 \nar.1.zehl               -0.659794   0.095464 \nar.1.scho               -0.865145   0.132413 \nar.1.trko               -1.002428   0.165175 \nend.sin(2 * pi * t/52)   0.121489   0.024384 \nend.cos(2 * pi * t/52)   0.923900   0.024745 \nend.1.chwi               1.181472   0.088438 \nend.1.frkr               0.604251   0.081671 \nend.1.lich               1.020524   0.089385 \nend.1.mahe               0.791906   0.089509 \nend.1.mitt               1.010631   0.081603 \nend.1.neuk               0.823126   0.097049 \nend.1.pank               1.298509   0.104632 \nend.1.rein               0.791816   0.097056 \nend.1.span               0.648208   0.092789 \nend.1.zehl               1.297808   0.091415 \nend.1.scho               1.189950   0.093128 \nend.1.trko               0.997664   0.094204 \noverdisp                 0.197678   0.009884 \n\nLog-likelihood:   -8655.46 \nAIC:              17364.92 \nBIC:              17532.63 \n\nNumber of units:        12 \nNumber of time points:  307 \n\n# compute AIC\nAIC(fit2)\n\n[1] 17364.92\n\n\nWe check the Pearson residuals again, which now look resonable in terms of mean and variance:\n\n# compute Pearson residuals and check their mean and variance:\npr2 <- pearson_residuals(fit2)\ncolMeans(pr2)\n\n        chwi         frkr         lich         mahe         mitt         neuk \n 0.020061368  0.025144794 -0.010413219  0.003481020 -0.004801394 -0.006139166 \n        pank         rein         span         zehl         scho         trko \n-0.012998766 -0.004079849 -0.001710578 -0.022435249 -0.007614639 -0.016314666 \n\napply(pr1, 2, sd)\n\n     chwi      frkr      lich      mahe      mitt      neuk      pank      rein \n0.8969573 0.8394516 0.9984702 0.9348625 0.9594755 0.8998828 1.1386447 0.9955265 \n     span      zehl      scho      trko \n0.9606598 1.2538557 1.0589218 0.9730101 \n\n\nNote that we could also add district-specific dispersion parameters \\(\\psi_i\\) by setting family = \"NegBinM\", but for simplicity we stick with the shared overdispersion parameter.\n\n\nAdding dependence between districts.\nFor now we have been modelling each district separately (while sharing some strength by pooling parameters). In a next step we add dependencies via the ne (neighbourhood) component. There are different ways of specifying the coupling between districts. The neighbourhood slot of the sts object contains the necessary information on the geographical disposition.\nWe now fit a model where only direct neighbours affect each other, setting \\[\\begin{align}\nY_{it} \\ \\mid \\ \\ \\text{past} & \\sim \\text{NegBin}(\\mu_{it}, \\psi)\\\\\n\\mu_{it} & = \\nu_{it} + \\lambda_i Y_{i, t - 1} + \\phi_i \\sum_{j \\sim i} w_{ji} Y_{j, t - 1}.\n\\end{align}\\] Here, the relationship \\(\\sim\\) indicates direct neighbourhood. The weights \\(w_{ji}\\) can be chosen in two ways.\n\nif normalize == TRUE: \\(w_{ji} = 1/\\) number of neighbours of \\(j\\) if \\(i \\sim j\\), 0 else\nif normalize == FALSE: \\(w_{ji} = 1\\) if \\(i \\sim j\\), 0 else\n\nWe usually use normalize = TRUE, which is based on the intuition that each district splits up its infectious pressure equally among its neighbours. Note that in this formulation, the autoregression on past incidence from the same district and others are handled each on their own with separate parameters.\nIn a simple graphical ilustration with just three districts where district 2 is neighbouring both 1 and 3, but 1 and 3 are not neighbours, the model looks as follows. The grey lines indicate that these dependencies are typically weaker than the ones to previous values from the same district.\n\n\n# The default setting for the ne component is to use weights \n# neighbourhood(stsObj) == 1 (see ?hhh4).\nneighbourhood(noroBE) == 1\n\n      chwi  frkr  lich  mahe  mitt  neuk  pank  rein  span  zehl  scho  trko\nchwi FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\nfrkr FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE\nlich FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\nmahe FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\nmitt  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\nneuk FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\npank FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\nrein  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE\nspan  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\nzehl  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\nscho  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\ntrko FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n\n# this is because historically neighbourhood matrices were just binary\n# however, the path distances are coded in a way that direct neighbours have \n# distance 1, meaning that there is no need to modify the neighbourhood matrix\n\n##################################################################\n# Model 3:\nctrl3 <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              ar = list(f = ~ 0 + fe(1, unitSpecific = TRUE)),\n              ne = list(f = ~ 0 + fe(1, unitSpecific = TRUE), normalize = TRUE), \n              #  now added ne component to reflect cross-district dependencies\n              family = \"NegBin1\",\n              subset = subset_fit)\n# normalize = TRUE normalizes weights by the number of neighbours of the \n# exporting district\nfit3 <- hhh4(noroBE, ctrl3)\n\n# alternative: use update\n# fit3 <- update(fit2, ne = list(f = ~ 0 + fe(1, unitSpecific = TRUE),\n#                                weights = neighbourhood(noroBE) == 1,  # little bug?\n#                                normalize = TRUE))\nsummary(fit3) # parameters for different districts are quite different\n\n\nCall: \nhhh4(stsObj = noroBE, control = ctrl3)\n\nCoefficients:\n                        Estimate   Std. Error\nar.1.chwi               -2.984424   1.350606 \nar.1.frkr               -1.546018   0.255272 \nar.1.lich               -1.481704   0.269704 \nar.1.mahe               -1.042180   0.165513 \nar.1.mitt               -1.684145   0.325195 \nar.1.neuk               -1.178880   0.203516 \nar.1.pank               -0.950771   0.162474 \nar.1.rein               -0.739190   0.113666 \nar.1.span               -1.030189   0.159685 \nar.1.zehl               -0.700978   0.109133 \nar.1.scho               -1.140107   0.188365 \nar.1.trko               -0.954548   0.156077 \nne.1.chwi               -1.556838   0.229854 \nne.1.frkr               -2.105432   0.303360 \nne.1.lich               -1.333358   0.275089 \nne.1.mahe               -2.082596   1.001100 \nne.1.mitt               -1.382118   0.285452 \nne.1.neuk               -1.005697   0.308230 \nne.1.pank               -0.594043   0.265078 \nne.1.rein               -1.531222   0.365439 \nne.1.span               -1.496264   0.237766 \nne.1.zehl               -1.197674   0.519991 \nne.1.scho               -1.174415   0.268474 \nne.1.trko               -2.805666   1.088384 \nend.sin(2 * pi * t/52)   0.013954   0.037300 \nend.cos(2 * pi * t/52)   0.918633   0.033880 \nend.1.chwi               0.817175   0.143907 \nend.1.frkr               0.142450   0.200990 \nend.1.lich               0.636253   0.176280 \nend.1.mahe               0.715465   0.135111 \nend.1.mitt               0.541823   0.187192 \nend.1.neuk               0.444922   0.175794 \nend.1.pank               0.867238   0.188664 \nend.1.rein               0.349285   0.222273 \nend.1.span               0.143326   0.183118 \nend.1.zehl               1.068714   0.140279 \nend.1.scho               0.730291   0.182337 \nend.1.trko               0.866186   0.143240 \noverdisp                 0.182920   0.009464 \n\nLog-likelihood:   -8596.56 \nAIC:              17271.12 \nBIC:              17513.37 \n\nNumber of units:        12 \nNumber of time points:  307 \n\nAIC(fit3)\n\n[1] 17271.12\n\n\nPlotting the model fits, we see that for certain districts the neighbourhood component (orange) is very important (chwi), for others negligible (trko). This may be due to poor identifiability (after all, the autoregressive terms from the same and other districts are strongly correlated).\n\nplot(fit3, unit = 1:12, par.settings = list(mfcol = c(6, 2)))\n\n\n\n\n\n\nAdding a power law for spatial dependence.\nThe above formulation requires a lot of parameters as the autoregressions on the same and other regions are handled separately, and only takes into account first neighbours. A more parsimonious model also alowing for dependencies between indirect neighbours is a power-law formulation, where we set \\[\\begin{align}\n\\mu_{it} & = \\nu_{it} + \\phi_i \\sum_{j = 1}^K w_{ji} Y_{j, t - 1}\n\\end{align}\\] with weights \\(w_{ji}\\) defined as\n\\[\nw_{ji} = \\begin{cases} (o_{ji} + 1)^{-\\rho} & \\text{ if } \\text{normalize == FALSE}\\\\\n\\frac{(o_{ji} + 1)^{-\\rho}}{\\sum_{i = 1}^K (o_{jk} + 1)^{-\\rho}}  & \\text{ if } \\text{normalize == TRUE} \\end{cases},\n\\]\nwhere \\(o_{ji}\\) is the path distance between districts \\(j\\) and \\(i\\).\nReturning to our simple illustration with three districts, this step adds some additional connections between indirect neighbours (districts 1 and 3). They are shown in light gray as by construction of the model they are weaker than those between neighbouring districts.\n\n\n##################################################################\n# Model 4\n\n# For the next model version we will formally incorporate the autoregressive\n# copoenent into the neighbourhood component\n# (i.e. do no longer treat autoregression on the same district separately). \n# This can be done as follows.\n# First we need to adapt the neighbourhood matrix, shifting by one\nnoroBE_power <- noroBE\nnoroBE_power@neighbourhood <- noroBE@neighbourhood + 1\n\n# new control argument:\nctrl4 <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              # note: line ar = ... is removed\n              ne = list(f = ~0 + fe(1, unitSpecific = TRUE),\n                        weights = W_powerlaw(maxlag=5, normalize = TRUE,\n                                             log = TRUE)), # this is new\n              family = \"NegBin1\",\n              subset = subset_fit)\n# normalize = TRUE normalizes weights by the number of neighbours of the\n# exporting district\n# log = TRUE means optimization will be done on a log scale, ensuring \n# positivity of the decay parameter (which is desirable)\nfit4 <- hhh4(noroBE_power, ctrl4)\nAIC(fit4)\n\n[1] 17233.06\n\n\nThis yields a quite drastic improvement in AIC. We can visualize the weights \\(w_{ji}\\) as a function of the path dependence \\(o_{ji}\\)\n\n# visualize the nighbourhood weights:\nplot(fit4, type = \"neweights\", main = \"Weights by path distance\\n (note: 1 is same region and 2 is direct neighbours here)\")\n\n\n\n\nWhen plotting the model fit we note that there is now no autoregressive component (blue).\n\nplot(fit4, unit = 1:12, par.settings = list(mfcol = c(6, 2)))\n\n\n\n# note there is now no autoregressive component in the fit plots\n\n\n\nAdding seasonality to the epidemic component.\nIncluding seasonality in the ar and ne components when both are present can lead to identifiability issues. In the more parsimonious model where only the ne component is present, however, including seasonal terms here can considerably improve the model fit. This is indeed the case in our example.\n\n##################################################################\n# Model 5:\nctrl5 <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              # now adding seasonality to the ne component:\n              ne = list(f =  addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1),\n                        weights = W_powerlaw(maxlag=5, normalize = TRUE,\n                                             log=TRUE)), # this is new\n              family = \"NegBin1\",\n              subset = subset_fit)\nfit5 <- hhh4(noroBE_power, ctrl5)\nAIC(fit5)\n\n[1] 17205.41\n\n\nWe also consider the autocorrelation of the Pearson residuals. For several regions there are pronounced residual autocorrelations at lags 2 or 3, indicating that there is additional information to be exploited.\n\n# compute Pearson residuals:\npr5 <- pearson_residuals(fit4)\npar(mfrow = c(3, 4))\nfor(unit in colnames(pr5)){\n  acf(pr5[, unit], lag.max = 5, ylim = c(-0.3, 0.3), main = unit)\n}\n\n\n\n\n\n\nAdding higher-order lags.\nThe hhh4addon package contains functionality to add higher-order lags to the endemic-epidemic model,\n\\[\n\\mu_{it} = \\nu_{it} + \\phi_i \\sum_{j = 1}^K \\sum_{d = 1}^D u_d w_{ji} Y_{j, t - d}.\n\\]\nLag weights are (usually) governed by a single parameter, which is fed into a function returning the weights. The package contains the several parameterizations:\n\ngeometric: \\(\\tilde{u}_d = \\alpha(1 - \\alpha)^{d - 1}\\)\nPoisson: \\(\\tilde{u}_d = \\alpha^{d - 1}\\exp(-\\alpha)/(d - 1)!\\)\n\nBefore entering into \\(\\eqref{eq:higher_order}\\) the lag weights are standardized so they sum up to one, setting \\(u_d = \\tilde{u}_d / \\sum_{i = 1}^D \\tilde{u}_{i}\\).\nReturning a last time to our toy illustration of three districts, this step adds a whole lot of (typically week) connections between non-neighbouring time points.\n\n\n# check out examples of the different lag types:\n# geometric:\n(g <- geometric_lag(par_lag = 1.5, min_lag = 1, max_lag = 5))\n\n[1] 0.8177396888 0.1491765911 0.0272136178 0.0049644585 0.0009056439\n\n# first weight corresponds to exp(1.5)/(1 + exp(1.5))\n# 5 is also the default number of lags\n\n# Poisson:\n(p <- poisson_lag(par_lag = 0.8, min_lag = 1, max_lag = 5))\n\n[1] 0.1168028 0.2599493 0.2892639 0.2145896 0.1193945\n\n# weights correspond to dpois(0:4, exp(0.8))/sum(dpois(0:4, exp(0.8)))\n\npar(mfrow = 1:2)\nplot(g, xlab = \"lag\", ylab = \"weight\", ylim = 0:1, type = \"h\", \n     main = \"Geometric weights\")\nplot(p, xlab = \"lag\", ylab = \"weight\", ylim = 0:1, type = \"h\", \n     main = \"Poisson weights\")\n\n\n\n# A two-point distribution and a triangular distribution are also available.\n# Users can also provide their own weighting functions.\n\nThe parameterizations of these functions are chosen such that any value from the real line can be provided to them, which will facilitate optimization in a next step\n\nThe lag weihts can be pre-specified, but most of the time we will estimate them from the data rather than pre-specifying them. This can be done via the (poorly named) function profile_par_lag. We use geometric lags (which is also the default).\n\n##################################################################\n# Model 7\nctrl7 <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              # note: line ar = ... is removed\n              ne = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                              S = 1),\n                        weights = W_powerlaw(maxlag=5, normalize = TRUE,\n                                             log=TRUE)), # this is new\n              family = \"NegBin1\",\n              subset = subset_fit,\n              # (no specification of par_lag in the control)\n              funct_lag = geometric_lag)\n# now use profile_par_lag (applies a profile likelihood procedure to estimate\n# the lag decay parameter)\nfit7 <- profile_par_lag(noroBE_power, ctrl6)\n\nWarning in profile_par_lag(noroBE_power, ctrl6): Your control list contains a\npar_lag element. This is ignored by profile_par_lag. To fix par_lag rather than\nestimating it from the data use the function hhh4lag.\n\nAIC(fit7)\n\n[1] 17138.12\n\n\nWe can plot the weights as estimated from the data:\n\n# plot the weights assigned to the different lags:\npar(mfrow = 1:2)\nplot(fit7$distr_lag, type = \"h\", xlab  = \"lag\",\n     ylab = \"weight\", ylim = 0:1)\n\n\n\n\nAnd consider the Pearson residuals, which look less problematic than before (at least that’s what I like to tell myself).\n\n# check Pearson residuals\npr7 <- pearson_residuals(fit6)\npar(mfrow = c(3, 4))\nfor(unit in colnames(pr7)){\n  acf(pr7[, unit], lag.max = 5, ylim = c(-0.3, 0.3), main = unit)\n}"
  },
  {
    "objectID": "tutorials/multivariate.html#one-step-ahead-forecasting",
    "href": "tutorials/multivariate.html#one-step-ahead-forecasting",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "One-step-ahead forecasting",
    "text": "One-step-ahead forecasting\nThe packages contain functionality to imitate one-step-ahead (out-of-sample) forecasts retrospectively. To this end, the model is sequentially re-fitted, always including all data up to a given week. Then a one-week-ahead plug-in prediciton is obtained for the next week. We apply this to obtain predictions for weeks 313 through 364 which we had excluded from model fitting so far. Subsequently, you can use scores to evaluate the one-step-ahead predictions using several different statistical metrics. We compute the following:\n\nthe logarithmic score, also called negative predictive log-likelihood (informal explanation: this reflects how likely the observed outcomes were under your predictions)\nthe CRPS (informal explanation: this describes “how far” the observations were from the predictions you issued)\n\nFor both lower values` are better.\n\n##################################################################\n# one-step-ahead forecasting: generate forecasts sequentially\n# compare models 2, 4 and 7\nlog2 <- capture.output(owa2 <- oneStepAhead(fit2, tp = c(312, 363)))\nlog4 <- capture.output(owa4 <- oneStepAhead(fit4, tp = c(312, 363)))\nrm(log2, log4)\n# the weird capture.output formulation is needed to suppress \n# numerous cat() messages.\n# you could also just use\n# owa2 <- oneStepAhead(fit2, tp = c(312, 363))\nowa7 <- oneStepAhead_hhh4lag(fit7, tp = c(312, 363))\n\nLag weights are not re-estimated for the one-step-ahead forecasts (set refit_par_lag = TRUE to re-fit them).\n\n# the return objects contain predictions, observations and a few other things:\nhead(owa7$pred)\n\n         chwi     frkr      lich     mahe      mitt      neuk     pank     rein\n313 10.143891 5.997015  8.315079 7.550797 12.276676  9.072281 18.06990 12.35556\n314 10.052363 6.133326  9.568715 7.025105 12.332588 11.455435 20.52966 10.66938\n315 10.837813 6.638302 10.085745 8.250180 10.817444 10.246092 22.11388 17.05596\n316  9.737658 6.614877 16.126879 8.251785  8.842089  8.346344 24.43109 14.89341\n317  8.017484 5.761749 10.387913 6.531485  6.529512  7.030808 20.62327 11.56332\n318  9.328702 7.435577 13.008535 6.949797  8.054874  9.280143 20.39342 11.20380\n         span     zehl     scho     trko\n313 13.161304 20.62583 19.19221 7.951694\n314 10.103430 23.18901 18.21469 8.461257\n315  8.692397 24.61323 21.20857 8.030405\n316  7.391036 19.24499 17.81345 7.817094\n317 10.879236 19.91117 14.02415 6.814946\n318 11.205468 19.68513 16.13912 8.639782\n\nhead(owa7$observed)\n\n    chwi frkr lich mahe mitt neuk pank rein span zehl scho trko\n313   10    4   10    5   17   16   23    5    7   24   17    5\n314   12    8    9   10    6    8   23   26    3   24   28    3\n315   11    7   31    7    3    5   28   14    4   14   18    2\n316    5    8    6    5    0    5   22    8   18   20   12    4\n317   13   16   20    6    7   11   20    8   14   17   18    6\n318    7    8    7    5    3   16    9   20   12   22   31   11\n\n# plot one-step-ahead point forecasts:\nplot(noroBE@observed[313:364, 1], pch = 20, xlab = \"week\", ylab = \"value\")\nlines(owa2$pred[, 1], type = \"l\", col = 2)\nlines(owa4$pred[, 1], type = \"l\", col = 4)\nlines(owa7$pred[, 1], type = \"l\", col = 6)\n\n\n\n# compute and summarize scores:\ncolMeans(scores(owa2, which = c(\"logs\", \"rps\")))\n\n    logs      rps \n2.506608 2.091024 \n\ncolMeans(scores(owa4, which = c(\"logs\", \"rps\")))\n\n    logs      rps \n2.492431 2.066390 \n\ncolMeans(scores(owa7, which = c(\"logs\", \"rps\")))\n\n    logs      rps \n2.475311 2.042008 \n\n\nWe can see that the more complex model formulations also translate to improved predictive performance (all scores being negatively oriented; see ?scores)."
  },
  {
    "objectID": "tutorials/multivariate.html#predictive-moments-at-longer-forecast-horizons.",
    "href": "tutorials/multivariate.html#predictive-moments-at-longer-forecast-horizons.",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "Predictive moments at longer forecast horizons.",
    "text": "Predictive moments at longer forecast horizons.\nThe hhh4addon package also contains functions to compute predictive moments (means, variances, autocorrelations) for longer time horizons as well as marginal/stationary moments of the fitted model.\n\n##################################################################\n# longer-term predictive moments can be computed using predictive_moments:\n# predictive moments 10 weeks ahead:\npred_mom7 <- predictive_moments(fit7, t_condition = max(subset_fit), lgt = 10)\n# print some predictive means:\nhead(pred_mom7$mu_matrix[, 1:6])\n\n           chwi     frkr      lich     mahe      mitt     neuk\nt=313 10.143891 5.997015  8.315079 7.550797 12.276676 9.072281\nt=314 10.256036 6.200316  9.202886 7.794306 11.403386 9.483784\nt=315 10.224376 6.285932  9.759873 7.944234 10.889366 9.757613\nt=316 10.077085 6.281429 10.044002 7.998614 10.518741 9.877621\nt=317  9.833192 6.199311 10.132328 7.930412 10.185662 9.836843\nt=318  9.529173 6.051469 10.026521 7.766129  9.843626 9.705029\n\n# plot:\nplot(fit7)\nfanplot_prediction(pred_mom7, add = TRUE,\n                   probs = c(0.05, 0.15, 0.25, 0.75, 0.85, 0.95),\n                   pt.col = \"black\")\n\n\n\n# note: the plot is based on a negative binomial approximation of the \n# predictive distributions.\n\n# stationary/marginal moments are implemented, too (but don't always exist):\nstat7 <- stationary_moments(fit6)\nfanplot_stationary(stat7, unit = 4)"
  },
  {
    "objectID": "tutorials/multivariate.html#over-to-you",
    "href": "tutorials/multivariate.html#over-to-you",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "Over to you…",
    "text": "Over to you…\nYou are now invited to perform an analysis of rotavirus in Berlin along the lines of the above. The data, which are structured the same way as the norovirus data. Your “goal” is to formulate a model which will generate good one-step-ahead forecasts (we’ll use logS as the main outcome, which is the so-called logarithmic score, equivalent to the negative predictive log-likelihood). You can load the data via\n\ndata(\"rotaBE\")\n# if you don't have the hhh4addon package installed you can use:\n# load(\n# url(\"https://github.com/cmmid/hhh4-workshop/blob/main/tutorial2_multivariate/data/rotaBE.rda?raw=true\")\n# )\n\nYour code should look somewhat like the following:\n\nLook at the data to get an idea of what it looks like.\nFormulate and refine a control argument. To imitate a forecasting setting it makes sense to only use observations up to week 312 here (though you’re free to play around with whatever subsets of the data you like).\n\n\nctrl <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              ar = list(f = ~ 0 + fe(1, unitSpecific = TRUE)),\n              family = \"NegBin1\",\n              subset = 6:312)\n\n\nFit the model using hhh4 or profile_par_lag.\n\n\nyour_fit <- hhh4(rotaBE, ctrl)\n\n\nRun the following (remove comments:\n\n\n# owa <- oneStepAhead(your_fit, tp = c(312, 363))\n# or: owa <- oneStepAhead_hhh4(your_fit, tp = c(312, 363))\n# if you have used profile_par_lag \n# colMeans(scores(owa), which = c(\"logs\", \"rps\"))\n#     logs       rps\n# 1.931189  1.639351\n\nIf you like you can post your results here (https://github.com/cmmid/hhh4-workshop/issues/1) or on the chat.\nHere are some model aspects you can play around with:\n\nHigher-order seasonal terms (as in a Fourier series) can be included by setting S = 2 or higher in the addSeason2formula function.\nLinear time trends can be included by adding + t to a formula, e.g. f = addSeason2formula($\\sim$ 0 + t + fe(1, unitSpecific = TRUE), S = 1)\nRotavirus transmission is known to be linked to temperature (https://doi.org/10.1098/rspb.2009.1755). We therefore uploaded a time series of weekly mean temperature values in Berlin (average of daily temperatures at 2pm, lagged by one week) to the GitHub repo of the workshop. These (or a transformation) can be used a a covariate.\n\n\n# get temperature data:\ndata_temperature <-\n  read.csv(paste0(\"https://raw.githubusercontent.com/cmmid/hhh4-workshop/\", \n                  \"main/tutorial2_multivariate/data/temperature_berlin.csv\"))\ntemperature <- data_temperature$temperature7d\n# your formula could look as follows:\nctrl <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                              S = 1)),\n             ar = list(f = ~ 0 + temperature + fe(1, unitSpecific = TRUE)),\n             family = \"NegBin1\",\n             subset = 6:312)\n# though that is not necessarily a very smart way of using the covariate"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "",
    "text": "Host: Centre for Mathematical Modelling of Infectious Diseases at the London School of Hygiene & Tropical Medicine"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Background",
    "text": "Background\nSpatio-temporal models are used extensively to describe infectious disease dynamics while accounting for spatial heterogeneity in transmission risk. The Endemic-Epidemic modelling framework is a class of spatio-temporal models used for analysing subnational case count data implemented in the R package surveillance. It is characterised by its flexibility and ease of use, and it can easily integrate various data streams. It has been used to analyse outbreak dynamics in a wide range of settings, and applied to various diseases (including COVID-19, dengue, measles, and foot and mouth disease). This day-long workshop will provide an overview of the objectives and scope of Endemic-Epidemic models through a hands-on tutorial, followed by several short presentations and discussion on recent applications and future extensions of the framework."
  },
  {
    "objectID": "index.html#organisers",
    "href": "index.html#organisers",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Organisers",
    "text": "Organisers\n\nAlexis Robert\nEmily Nightingale\nLloyd Chapman\nSebastian Funk"
  },
  {
    "objectID": "index.html#introduction-tutorial-and-summary",
    "href": "index.html#introduction-tutorial-and-summary",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Introduction, tutorial and summary",
    "text": "Introduction, tutorial and summary\n\nJohannes Bracher\nLeonhard Held\nMaria Bekker-Nielsen Dunbar\nSebastian Meyer"
  },
  {
    "objectID": "applications/session2.html",
    "href": "applications/session2.html",
    "title": "Short talks on applications: session 2",
    "section": "",
    "text": "Presenter\nTitle\nBibliography\nVideo\n\n\n\n\nAlastair Munro\nPertussis in England & Wales\nMunro et al. (2021)\n\n\n\nAlexis Robert\nMeasles in France\nRobert et al. (2021)\n\n\n\nEmily Nightingale\nVisceral leishmaniasis in India\nNightingale et al. (2020)\n\n\n\nCici Bauer\nHand, foot and mouth disease in China\nBaur & Wakefield (2018)"
  },
  {
    "objectID": "applications/session1.html",
    "href": "applications/session1.html",
    "title": "Short talks on applications: session 1",
    "section": "",
    "text": "Presenter\nTitle\nBibliography\nVideo\n\n\n\n\nMathilde Grimée\nCOVID-19 in Switzerland\nGrimée et al. (2021)\n\n\n\nTrang Nguyen\nMeasles in Vietnam\n\n\n\n\nClaudio Fronterre\nCOVID-19 in England\nFronterre et al. (2020)\n\n\n\nJohannes Bracher\nFitting endemic-epidemic models to underreported disease surveillance counts\nBracher & Held (2020)"
  }
]