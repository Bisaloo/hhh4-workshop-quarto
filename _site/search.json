[
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Background",
    "text": "Background\nSpatio-temporal models are used extensively to describe infectious disease dynamics while accounting for spatial heterogeneity in transmission risk. The Endemic-Epidemic modelling framework is a class of spatio-temporal models used for analysing subnational case count data implemented in the R package surveillance. It is characterised by its flexibility and ease of use, and it can easily integrate various data streams. It has been used to analyse outbreak dynamics in a wide range of settings, and applied to various diseases (including COVID-19, dengue, measles, and foot and mouth disease). This day-long workshop will provide an overview of the objectives and scope of Endemic-Epidemic models through a hands-on tutorial, followed by several short presentations and discussion on recent applications and future extensions of the framework."
  },
  {
    "objectID": "index.html#organisers",
    "href": "index.html#organisers",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Organisers",
    "text": "Organisers\n\nAlexis Robert\nEmily Nightingale\nLloyd Chapman\nSebastian Funk"
  },
  {
    "objectID": "index.html#introduction-tutorial-and-summary",
    "href": "index.html#introduction-tutorial-and-summary",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Introduction, tutorial and summary",
    "text": "Introduction, tutorial and summary\n\nJohannes Bracher\nLeonhard Held\nMaria Bekker-Nielsen Dunbar\nSebastian Meyer"
  },
  {
    "objectID": "index.html#material-for-tutorial",
    "href": "index.html#material-for-tutorial",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Material for tutorial",
    "text": "Material for tutorial\nThe code for the tutorial part of the workshop can be found here in the “tutorial” folders. In order to participate in the tutorial you will need to have R installed on your computer and to install the surveillance and hhh4addon R packages. Instructions for how to do this can be found in the README here."
  },
  {
    "objectID": "index.html#recordings",
    "href": "index.html#recordings",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Recordings",
    "text": "Recordings\nShort talks - session 1\nShort talks - session 2"
  },
  {
    "objectID": "index.html#applications",
    "href": "index.html#applications",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Applications",
    "text": "Applications\n\nSession 1\n[Recording]\n\nMathilde Grimée: COVID-19 in Switzerland\nTrang Nguyen: Measles in Vietnam\nClaudio Fronterre: COVID-19 in England\nJohannes Bracher: Fitting endemic-epidemic models to underreported disease surveillance counts\n\n\n\nSession 2\n[Recording]\n\nAlastair Munro: Pertussis in England & Wales\nAlexis Robert: Measles in France\nEmily Nightingale: Visceral leishmaniasis in India\nCici Bauer: Hand, foot and mouth disease in China"
  },
  {
    "objectID": "index.html#final-programme-all-in-london-time",
    "href": "index.html#final-programme-all-in-london-time",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Final programme (all in London time)",
    "text": "Final programme (all in London time)\n:::{.column-page}\n\n\n\n\n\n\n\n\n\nTime\n\nPresenter\nMaterial\n\n\n\n\n9:30\nWelcome\n\n\n\n\n9:35\nIntroduction\nLeonhard Held\n\n\n\n10:00\nR tutorial: using hhh4\nSebastian Meyer\nTutorial 1\n\n\n\n\nJohannes Bracher\nTutorial 2\n\n\n\n\nMaria Bekker-Nielsen Dunbar\nTutorial 3\n\n\n12:30\nLunch break\n\n\n\n\n13:30\nShort talks on applications\nMathilde Grimée  Trang Nguyen  Claudio Fronterre  Johannes Bracher\n\n\n\n14:30\nCoffee\n\n\n\n\n15:00\nShort talks on applications\nAlastair Munro  Alexis Robert  Emily Nightingale  Cici Bauer\n\n\n\n16:00\nDiscussion / Outlook\n\n\n\n\n16:30\nClosing\n\n\n\n\n\n:::{.column-page}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Workshop on the Endemic-Epidemic framework for infectious disease modelling",
    "section": "Tutorials",
    "text": "Tutorials"
  },
  {
    "objectID": "tutorials/multivariate.html#whats-the-plan",
    "href": "tutorials/multivariate.html#whats-the-plan",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "What’s the plan?",
    "text": "What’s the plan?\nIn this part of the tutorial you will learn how to build multivariate hhh4 models. We will proceed as follows:\n\nI will talk you through an example analysis of norovirus in the twelve districts of Berlin, adding complexity to an hhh4 model step by step.\nThen you will be able to use the code chunks from my example to analyse a data set on rotavirus in Berlin which has exactly the same structure as the norovirus data.\nWe’ll try to be available for questions while you are working on your analysis (as time and the number of participants permits).\nIf time permits we will go through some of your work at the end.\nWe are using these teaching materials for the first time, so they are likely to contain some errors and bugs – don’t hesitate to ask if something seems puzzling."
  },
  {
    "objectID": "tutorials/multivariate.html#repetition-installing-packages",
    "href": "tutorials/multivariate.html#repetition-installing-packages",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "Repetition: Installing packages",
    "text": "Repetition: Installing packages\nWe require the packages surveillance and hhh4addon, the latter containing some additional functionality for hhh4 models. It can be installed directly from GitHub using the remotes package (you may already have done that for the first part of the tutorial, but the necessary code is included here to make this document self-contained). ::: {.cell}\n# load packages:\n# options(warn = -1)\nlibrary(surveillance)\n\nLoading required package: sp\n\n\nLoading required package: xtable\n\n\nThis is surveillance 1.20.0. For overview type 'help(surveillance)'.\n\n# to install hhh4addon\n# install.packages(\"remotes\")\n# library(remotes)\n# remotes:::install_github(\"jbracher/hhh4addon\", build_vignettes = TRUE)\nlibrary(hhh4addon)\n\nLoading required package: polyCub\n\n\nThis is a development version of hhh4addon. It modifies and extends functionalities of surveillance:hhh4.\n\n\n\nAttaching package: 'hhh4addon'\n\n\nThe following object is masked from 'package:surveillance':\n\n    decompose.hhh4\n\n:::"
  },
  {
    "objectID": "tutorials/multivariate.html#example-data",
    "href": "tutorials/multivariate.html#example-data",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "Example data",
    "text": "Example data\nWe will use two data sets provided with the hhh4addon package (original data from Robert Koch Institute). The noroBE data contain weekly counts confirmed of norovirus cases in the twelve districts of Berlin, 2011–2017. The rotaBE data contain the same for rotavirus. In this note the norovirus data are analysed, which as an exercise you will be able to apply to the rotavirus data.\n\n# get the data\ndata(\"noroBE\")\n# if you don't have the hhh4addon package installed you can use:\n# load(\n#   url(\"https://github.com/cmmid/hhh4-workshop/blob/main/tutorial2_multivariate/data/noroBE.rda?raw=true\")\n# )\n\nWe start by plotting the temporal and spatial distribution of the data:\n\n# look at data:\n# time series:\nplot(noroBE, ylim = c(0, 40))\n\n\n\n# map:\n# as population(noroBE) contains population fractions rather than raw\n# population sizes setting population = 100,000/<population of Berlin>\n# will yield cumulative incidence per 100,000 inhabitants; see ?stsplot_space\nplot(noroBE, observed ~ unit, population = 100000/3500000, labels = TRUE)\n\n\n\n\nWe moreover take a look at the slot neighbourhood of the sts object. In this case it contains path distances between the districts: e.g., chwi and frkr have a distance of 2 as you need to go through mitt or scho.\n\n# check the neighbourhood structure, which is stored in the @neighbourhood slot \n# of the sts object:\nnoroBE@neighbourhood\n\n     chwi frkr lich mahe mitt neuk pank rein span zehl scho trko\nchwi    0    2    3    4    1    2    2    1    1    1    1    3\nfrkr    2    0    1    2    1    1    1    2    3    2    1    1\nlich    3    1    0    1    2    2    1    2    3    3    2    1\nmahe    4    2    1    0    3    2    2    3    4    4    3    1\nmitt    1    1    2    3    0    2    1    1    2    2    1    2\nneuk    2    1    2    2    2    0    2    3    3    2    1    1\npank    2    1    1    2    1    2    0    1    2    3    2    2\nrein    1    2    2    3    1    3    1    0    1    2    2    3\nspan    1    3    3    4    2    3    2    1    0    1    2    4\nzehl    1    2    3    4    2    2    3    2    1    0    1    3\nscho    1    1    2    3    1    1    2    2    2    1    0    2\ntrko    3    1    1    1    2    1    2    3    4    3    2    0"
  },
  {
    "objectID": "tutorials/multivariate.html#developing-a-multivariate-model-step-by-step",
    "href": "tutorials/multivariate.html#developing-a-multivariate-model-step-by-step",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "Developing a multivariate model step by step",
    "text": "Developing a multivariate model step by step\n\nStep 1: Seasonality in the endemic component, all parameters shared.\nWe start by formulating a very simple model where for districts \\(i = 1, \\dots, K\\) we assume \\[\\begin{align}\nY_{it} \\ \\mid \\ \\ \\text{past} & \\sim \\text{NegBin}(\\mu_{it}, \\psi)\\\\\n\\mu_{it} & = \\nu_t + \\lambda Y_{i, t - 1}\n\\end{align}\\] and \\[\n\\nu_{t} = \\alpha^\\nu + \\beta^\\nu \\sin(2\\pi t/52) + \\gamma^\\nu \\cos(2\\pi t/52).\n\\] Reminder: we refer to\n\n\\(\\nu_t\\) as the endemic component\n\\(\\lambda Y_{i, t - 1}\\) as the autoregressive or epidemic component\n\nIn a setting with just three districts the model could be illustrated as follows – case numbers the different districts are independent and only depend on the previous value in the same district.\n\nIn surveillance this model is specified as follows. Reminder: The hhh4 function takes the following arguments:\n\nan stsobject containing the data\na control argument describing the model formulation. This list contains a number of named arguments, the most important ones being:\n\nend: a specification of the endemic component (i.e., \\(\\nu\\)).\nar: a specification of the autoregressive component (\\(\\lambda\\); dependence on the previous value from the same district).\nne: a specification of the neighbourhood component (\\(\\phi\\); dependence on on previous value from other districts, to be introduced later). All of end, ar and ne are lists containing a formula and possibly some more elements, see ?hhh4\nfamily: the type of distribution used in the recursive model formulation (either \"Poisson\", \"NegBin1\", or \"NegBinM\").\nsubset: The subset of the time series to which the model shall be fit.\n\n\n\n# first define a subset of the data to which to fit (will be used in all model\n# fits to ensure comparability of AIC values):\nsubset_fit <- 6:(nrow(noroBE@observed) - 52)\n# we are leaving out the last year and the first 5 observations\n# the latter serves to ensure comparability to later model versions\n\n##################################################################\n# Model 1:\n# control list:\nctrl1 <- list(end = list(f = addSeason2formula(~1, S = 1)), # seasonality in end\n              # S = 1 means one pair of sine / cosine waves is included\n              ar = list(f = ~ 1), # no seasonality in ar\n              family = \"NegBin1\", # negative binomial (rather than Poisson)\n              subset = subset_fit)\n# fit model\nfit1 <- hhh4(noroBE, ctrl1)\n# summary of parameter estimates:\nsummary(fit1)\n\n\nCall: \nhhh4(stsObj = noroBE, control = ctrl1)\n\nCoefficients:\n                        Estimate  Std. Error\nar.1                    -0.72854   0.03453  \nend.1                    0.84477   0.02860  \nend.sin(2 * pi * t/52)   0.09024   0.02806  \nend.cos(2 * pi * t/52)   0.88574   0.02856  \noverdisp                 0.22957   0.01076  \n\nLog-likelihood:   -8780.82 \nAIC:              17571.64 \nBIC:              17602.7 \n\nNumber of units:        12 \nNumber of time points:  307 \n\n# you can set idx2Exp = TRUE to get all estimates on the exp-transformed scale\nsummary(fit1, idx2Exp = TRUE)\n\n\nCall: \nhhh4(stsObj = noroBE, control = ctrl1)\n\nCoefficients:\n                             Estimate  Std. Error\nexp(ar.1)                    0.48261   0.01667   \nexp(end.1)                   2.32745   0.06657   \nexp(end.sin(2 * pi * t/52))  1.09443   0.03071   \nexp(end.cos(2 * pi * t/52))  2.42477   0.06925   \noverdisp                     0.22957   0.01076   \n\nLog-likelihood:   -8780.82 \nAIC:              17571.64 \nBIC:              17602.7 \n\nNumber of units:        12 \nNumber of time points:  307 \n\n# get AIC to compare model fit to more complex models\nAIC(fit1)\n\n[1] 17571.64\n\n# visually inspect:\npar(mfcol = c(6, 2))\nplot(fit1, unit = 1:12, par.settings = list(mfcol = c(6, 2))) # look at all units\n\n\n\n# par.settings = list(mfcol = c(6, 2)) tells the function we want two columns\n# of plots\n\nThe grey area represents the endemic component (\\(\\nu_{t}\\)) of the fitted values with clearly visible seasonality. The blue area represents the autoregressive component.\nFor model inspection I like looking at Pearson residuals \\[\n\\frac{Y_{it} - \\hat{\\mu}_{it}}{\\hat{\\sigma}_{it}} = \\frac{Y_{it} - \\hat{\\mu}_{it}}{\\sqrt{\\hat{\\mu}_{it} + \\hat{\\mu}_{it}^2/\\psi}},\n\\] which are not currently implemented in surveillance, but can be computed using a little auxiliary function. For now I just look at their mean and variance per district. ::: {.cell hash=‘multivariate_cache/html/function_pr_ece5bc05b774c75d7dbb8fed9f4085c1’}\n# helper function to compute Pearson residuals:\npearson_residuals <- function(hhh4Obj){\n  # compute raw residuals:\n  response_residuals <- residuals(hhh4Obj, type = \"response\")\n  # compute standard deviations:\n  if (inherits(hhh4Obj, \"hhh4lag\")) {\n    sds <- sqrt(fitted(hhh4Obj) + \n                  fitted(hhh4Obj)^2/hhh4addon:::psi2size.hhh4lag(hhh4Obj))\n  } else {\n    sds <- sqrt(fitted(hhh4Obj) + \n                  fitted(hhh4Obj)^2/surveillance:::psi2size.hhh4(hhh4Obj))\n  }\n  # compute Pearson residuals:\n  pearson_residuals <- response_residuals/sds\n  return(pearson_residuals)\n}\n\npr1 <- pearson_residuals(fit1)\n# compute district-wise means and standard deviations:\ncolMeans(pr1)\n\n        chwi         frkr         lich         mahe         mitt         neuk \n-0.040128818 -0.354497174 -0.026596771 -0.149654100 -0.098957012 -0.139634691 \n        pank         rein         span         zehl         scho         trko \n 0.363677331 -0.006394507 -0.203281407  0.422776136  0.208763434 -0.023204469 \n\napply(pr1, 2, sd)\n\n     chwi      frkr      lich      mahe      mitt      neuk      pank      rein \n0.8969573 0.8394516 0.9984702 0.9348625 0.9594755 0.8998828 1.1386447 0.9955265 \n     span      zehl      scho      trko \n0.9606598 1.2538557 1.0589218 0.9730101 \n\n::: In each district, the Pearson residuals should have mean and standard deviations of approximately 0 and 1, respectively, which is clearly not the case. By sharing all parameters, fitted values are systematically larger than observed in some districts and smaller in others.\n\n\nAdding district-specific parameters.\nAs the districts are of quite different sizes and have different levels of incidence, it seems reasonable to use district-specific parameters and extend the model to \\[\\begin{align}\nY_{it} \\ \\mid \\ \\ \\text{past} & \\sim \\text{NegBin}(\\mu_{it}, \\psi)\\\\\n\\mu_{it} & = \\nu_{it} + \\lambda_i Y_{i, t - 1}\\\\\n\\nu_{it} & = \\alpha^\\nu_i + \\beta^\\nu \\sin(2\\pi t/52) + \\gamma^\\nu \\cos(2\\pi t/52)\n\\end{align}\\]\n\n##################################################################\n# Model 2:\n# We use fe(..., unitSpecific = TRUE) to add fixed effects for each unit,\n# in this case intercepts (1). Seasonality parameters are still shared\n# across districts\nctrl2 <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              ar = list(f = ~ 0 + fe(1, unitSpecific = TRUE)),\n              family = \"NegBin1\",\n              subset = subset_fit)\n# Note: ~0 is necessary as otherwise one would (implicitly) add two intercepts\n# unit-specific dispersion parameters could be added by setting family = \"NegBinM\"\nfit2 <- hhh4(noroBE, ctrl2)\n# check parameter estimates:\nsummary(fit2)\n\n\nCall: \nhhh4(stsObj = noroBE, control = ctrl2)\n\nCoefficients:\n                        Estimate   Std. Error\nar.1.chwi               -1.795086   0.402460 \nar.1.frkr               -1.259748   0.182296 \nar.1.lich               -1.087080   0.177651 \nar.1.mahe               -0.957952   0.145501 \nar.1.mitt               -1.335034   0.214060 \nar.1.neuk               -0.967629   0.159855 \nar.1.pank               -0.753360   0.129017 \nar.1.rein               -0.668191   0.103620 \nar.1.span               -0.823339   0.125950 \nar.1.zehl               -0.659794   0.095464 \nar.1.scho               -0.865145   0.132413 \nar.1.trko               -1.002428   0.165175 \nend.sin(2 * pi * t/52)   0.121489   0.024384 \nend.cos(2 * pi * t/52)   0.923900   0.024745 \nend.1.chwi               1.181472   0.088438 \nend.1.frkr               0.604251   0.081671 \nend.1.lich               1.020524   0.089385 \nend.1.mahe               0.791906   0.089509 \nend.1.mitt               1.010631   0.081603 \nend.1.neuk               0.823126   0.097049 \nend.1.pank               1.298509   0.104632 \nend.1.rein               0.791816   0.097056 \nend.1.span               0.648208   0.092789 \nend.1.zehl               1.297808   0.091415 \nend.1.scho               1.189950   0.093128 \nend.1.trko               0.997664   0.094204 \noverdisp                 0.197678   0.009884 \n\nLog-likelihood:   -8655.46 \nAIC:              17364.92 \nBIC:              17532.63 \n\nNumber of units:        12 \nNumber of time points:  307 \n\n# compute AIC\nAIC(fit2)\n\n[1] 17364.92\n\n\nWe check the Pearson residuals again, which now look resonable in terms of mean and variance:\n\n# compute Pearson residuals and check their mean and variance:\npr2 <- pearson_residuals(fit2)\ncolMeans(pr2)\n\n        chwi         frkr         lich         mahe         mitt         neuk \n 0.020061368  0.025144794 -0.010413219  0.003481020 -0.004801394 -0.006139166 \n        pank         rein         span         zehl         scho         trko \n-0.012998766 -0.004079849 -0.001710578 -0.022435249 -0.007614639 -0.016314666 \n\napply(pr1, 2, sd)\n\n     chwi      frkr      lich      mahe      mitt      neuk      pank      rein \n0.8969573 0.8394516 0.9984702 0.9348625 0.9594755 0.8998828 1.1386447 0.9955265 \n     span      zehl      scho      trko \n0.9606598 1.2538557 1.0589218 0.9730101 \n\n\nNote that we could also add district-specific dispersion parameters \\(\\psi_i\\) by setting family = \"NegBinM\", but for simplicity we stick with the shared overdispersion parameter.\n\n\nAdding dependence between districts.\nFor now we have been modelling each district separately (while sharing some strength by pooling parameters). In a next step we add dependencies via the ne (neighbourhood) component. There are different ways of specifying the coupling between districts. The neighbourhood slot of the sts object contains the necessary information on the geographical disposition.\nWe now fit a model where only direct neighbours affect each other, setting \\[\\begin{align}\nY_{it} \\ \\mid \\ \\ \\text{past} & \\sim \\text{NegBin}(\\mu_{it}, \\psi)\\\\\n\\mu_{it} & = \\nu_{it} + \\lambda_i Y_{i, t - 1} + \\phi_i \\sum_{j \\sim i} w_{ji} Y_{j, t - 1}.\n\\end{align}\\] Here, the relationship \\(\\sim\\) indicates direct neighbourhood. The weights \\(w_{ji}\\) can be chosen in two ways.\n\nif normalize == TRUE: \\(w_{ji} = 1/\\) number of neighbours of \\(j\\) if \\(i \\sim j\\), 0 else\nif normalize == FALSE: \\(w_{ji} = 1\\) if \\(i \\sim j\\), 0 else\n\nWe usually use normalize = TRUE, which is based on the intuition that each district splits up its infectious pressure equally among its neighbours. Note that in this formulation, the autoregression on past incidence from the same district and others are handled each on their own with separate parameters.\nIn a simple graphical ilustration with just three districts where district 2 is neighbouring both 1 and 3, but 1 and 3 are not neighbours, the model looks as follows. The grey lines indicate that these dependencies are typically weaker than the ones to previous values from the same district.\n\n\n# The default setting for the ne component is to use weights \n# neighbourhood(stsObj) == 1 (see ?hhh4).\nneighbourhood(noroBE) == 1\n\n      chwi  frkr  lich  mahe  mitt  neuk  pank  rein  span  zehl  scho  trko\nchwi FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\nfrkr FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE\nlich FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\nmahe FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\nmitt  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\nneuk FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\npank FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\nrein  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE\nspan  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\nzehl  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\nscho  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\ntrko FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n\n# this is because historically neighbourhood matrices were just binary\n# however, the path distances are coded in a way that direct neighbours have \n# distance 1, meaning that there is no need to modify the neighbourhood matrix\n\n##################################################################\n# Model 3:\nctrl3 <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              ar = list(f = ~ 0 + fe(1, unitSpecific = TRUE)),\n              ne = list(f = ~ 0 + fe(1, unitSpecific = TRUE), normalize = TRUE), \n              #  now added ne component to reflect cross-district dependencies\n              family = \"NegBin1\",\n              subset = subset_fit)\n# normalize = TRUE normalizes weights by the number of neighbours of the \n# exporting district\nfit3 <- hhh4(noroBE, ctrl3)\n\n# alternative: use update\n# fit3 <- update(fit2, ne = list(f = ~ 0 + fe(1, unitSpecific = TRUE),\n#                                weights = neighbourhood(noroBE) == 1,  # little bug?\n#                                normalize = TRUE))\nsummary(fit3) # parameters for different districts are quite different\n\n\nCall: \nhhh4(stsObj = noroBE, control = ctrl3)\n\nCoefficients:\n                        Estimate   Std. Error\nar.1.chwi               -2.984424   1.350606 \nar.1.frkr               -1.546018   0.255272 \nar.1.lich               -1.481704   0.269704 \nar.1.mahe               -1.042180   0.165513 \nar.1.mitt               -1.684145   0.325195 \nar.1.neuk               -1.178880   0.203516 \nar.1.pank               -0.950771   0.162474 \nar.1.rein               -0.739190   0.113666 \nar.1.span               -1.030189   0.159685 \nar.1.zehl               -0.700978   0.109133 \nar.1.scho               -1.140107   0.188365 \nar.1.trko               -0.954548   0.156077 \nne.1.chwi               -1.556838   0.229854 \nne.1.frkr               -2.105432   0.303360 \nne.1.lich               -1.333358   0.275089 \nne.1.mahe               -2.082596   1.001100 \nne.1.mitt               -1.382118   0.285452 \nne.1.neuk               -1.005697   0.308230 \nne.1.pank               -0.594043   0.265078 \nne.1.rein               -1.531222   0.365439 \nne.1.span               -1.496264   0.237766 \nne.1.zehl               -1.197674   0.519991 \nne.1.scho               -1.174415   0.268474 \nne.1.trko               -2.805666   1.088384 \nend.sin(2 * pi * t/52)   0.013954   0.037300 \nend.cos(2 * pi * t/52)   0.918633   0.033880 \nend.1.chwi               0.817175   0.143907 \nend.1.frkr               0.142450   0.200990 \nend.1.lich               0.636253   0.176280 \nend.1.mahe               0.715465   0.135111 \nend.1.mitt               0.541823   0.187192 \nend.1.neuk               0.444922   0.175794 \nend.1.pank               0.867238   0.188664 \nend.1.rein               0.349285   0.222273 \nend.1.span               0.143326   0.183118 \nend.1.zehl               1.068714   0.140279 \nend.1.scho               0.730291   0.182337 \nend.1.trko               0.866186   0.143240 \noverdisp                 0.182920   0.009464 \n\nLog-likelihood:   -8596.56 \nAIC:              17271.12 \nBIC:              17513.37 \n\nNumber of units:        12 \nNumber of time points:  307 \n\nAIC(fit3)\n\n[1] 17271.12\n\n\nPlotting the model fits, we see that for certain districts the neighbourhood component (orange) is very important (chwi), for others negligible (trko). This may be due to poor identifiability (after all, the autoregressive terms from the same and other districts are strongly correlated). ::: {.cell}\nplot(fit3, unit = 1:12, par.settings = list(mfcol = c(6, 2)))\n\n\n\n:::\n\n\nAdding a power law for spatial dependence.\nThe above formulation requires a lot of parameters as the autoregressions on the same and other regions are handled separately, and only takes into account first neighbours. A more parsimonious model also alowing for dependencies between indirect neighbours is a power-law formulation, where we set \\[\\begin{align}\n\\mu_{it} & = \\nu_{it} + \\phi_i \\sum_{j = 1}^K w_{ji} Y_{j, t - 1}\n\\end{align}\\] with weights \\(w_{ji}\\) defined as \\[\nw_{ji} = \\begin{cases} (o_{ji} + 1)^{-\\rho} & \\text{ if } \\text{normalize == FALSE}\\\\\n\\frac{(o_{ji} + 1)^{-\\rho}}{\\sum_{i = 1}^K (o_{jk} + 1)^{-\\rho}}  & \\text{ if } \\text{normalize == TRUE} \\end{cases},\n\\] where \\(o_{ji}\\) is the path distance between districts \\(j\\) and \\(i\\).\nReturning to our simple illustration with three districts, this step adds some additional connections between indirect neighbours (districts 1 and 3). They are shown in light gray as by construction of the model they are weaker than those between neighbouring districts.\n\n\n##################################################################\n# Model 4\n\n# For the next model version we will formally incorporate the autoregressive\n# copoenent into the neighbourhood component\n# (i.e. do no longer treat autoregression on the same district separately). \n# This can be done as follows.\n# First we need to adapt the neighbourhood matrix, shifting by one\nnoroBE_power <- noroBE\nnoroBE_power@neighbourhood <- noroBE@neighbourhood + 1\n\n# new control argument:\nctrl4 <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              # note: line ar = ... is removed\n              ne = list(f = ~0 + fe(1, unitSpecific = TRUE),\n                        weights = W_powerlaw(maxlag=5, normalize = TRUE,\n                                             log = TRUE)), # this is new\n              family = \"NegBin1\",\n              subset = subset_fit)\n# normalize = TRUE normalizes weights by the number of neighbours of the\n# exporting district\n# log = TRUE means optimization will be done on a log scale, ensuring \n# positivity of the decay parameter (which is desirable)\nfit4 <- hhh4(noroBE_power, ctrl4)\nAIC(fit4)\n\n[1] 17233.06\n\n\nThis yields a quite drastic improvement in AIC. We can visualize the weights \\(w_{ji}\\) as a function of the path dependence \\(o_{ji}\\) ::: {.cell hash=‘multivariate_cache/html/neweights_9f8a856358162b1f95c6e20b3a56aa51’}\n# visualize the nighbourhood weights:\nplot(fit4, type = \"neweights\", main = \"Weights by path distance\\n (note: 1 is same region and 2 is direct neighbours here)\")\n\n\n\n:::\nWhen plotting the model fit we note that there is now no autoregressive component (blue). ::: {.cell}\nplot(fit4, unit = 1:12, par.settings = list(mfcol = c(6, 2)))\n\n\n\n# note there is now no autoregressive component in the fit plots\n:::\n\n\nAdding seasonality to the epidemic component.\nIncluding seasonality in the ar and ne components when both are present can lead to identifiability issues. In the more parsimonious model where only the ne component is present, however, including seasonal terms here can considerably improve the model fit. This is indeed the case in our example.\n\n##################################################################\n# Model 5:\nctrl5 <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              # now adding seasonality to the ne component:\n              ne = list(f =  addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1),\n                        weights = W_powerlaw(maxlag=5, normalize = TRUE,\n                                             log=TRUE)), # this is new\n              family = \"NegBin1\",\n              subset = subset_fit)\nfit5 <- hhh4(noroBE_power, ctrl5)\nAIC(fit5)\n\n[1] 17205.41\n\n\nWe also consider the autocorrelation of the Pearson residuals. For several regions there are pronounced residual autocorrelations at lags 2 or 3, indicating that there is additional information to be exploited. ::: {.cell hash=‘multivariate_cache/html/pr5_b44f09097b4f37fda16ed02f38e6ef30’}\n# compute Pearson residuals:\npr5 <- pearson_residuals(fit4)\npar(mfrow = c(3, 4))\nfor(unit in colnames(pr5)){\n  acf(pr5[, unit], lag.max = 5, ylim = c(-0.3, 0.3), main = unit)\n}\n\n\n\n:::\n\n\nAdding higher-order lags.\nThe hhh4addon package contains functionality to add higher-order lags to the endemic-epidemic model,\n\\[\n\\mu_{it} = \\nu_{it} + \\phi_i \\sum_{j = 1}^K \\sum_{d = 1}^D u_d w_{ji} Y_{j, t - d}.\n\\]\nLag weights are (usually) governed by a single parameter, which is fed into a function returning the weights. The package contains the several parameterizations:\n\ngeometric: \\(\\tilde{u}_d = \\alpha(1 - \\alpha)^{d - 1}\\)\nPoisson: \\(\\tilde{u}_d = \\alpha^{d - 1}\\exp(-\\alpha)/(d - 1)!\\)\n\nBefore entering into \\(\\eqref{eq:higher_order}\\) the lag weights are standardized so they sum up to one, setting \\(u_d = \\tilde{u}_d / \\sum_{i = 1}^D \\tilde{u}_{i}\\).\nReturning a last time to our toy illustration of three districts, this step adds a whole lot of (typically week) connections between non-neighbouring time points.\n\n\n# check out examples of the different lag types:\n# geometric:\n(g <- geometric_lag(par_lag = 1.5, min_lag = 1, max_lag = 5))\n\n[1] 0.8177396888 0.1491765911 0.0272136178 0.0049644585 0.0009056439\n\n# first weight corresponds to exp(1.5)/(1 + exp(1.5))\n# 5 is also the default number of lags\n\n# Poisson:\n(p <- poisson_lag(par_lag = 0.8, min_lag = 1, max_lag = 5))\n\n[1] 0.1168028 0.2599493 0.2892639 0.2145896 0.1193945\n\n# weights correspond to dpois(0:4, exp(0.8))/sum(dpois(0:4, exp(0.8)))\n\npar(mfrow = 1:2)\nplot(g, xlab = \"lag\", ylab = \"weight\", ylim = 0:1, type = \"h\", \n     main = \"Geometric weights\")\nplot(p, xlab = \"lag\", ylab = \"weight\", ylim = 0:1, type = \"h\", \n     main = \"Poisson weights\")\n\n\n\n# A two-point distribution and a triangular distribution are also available.\n# Users can also provide their own weighting functions.\n\nThe parameterizations of these functions are chosen such that any value from the real line can be provided to them, which will facilitate optimization in a next step\n\nThe lag weihts can be pre-specified, but most of the time we will estimate them from the data rather than pre-specifying them. This can be done via the (poorly named) function profile_par_lag. We use geometric lags (which is also the default).\n\n##################################################################\n# Model 7\nctrl7 <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              # note: line ar = ... is removed\n              ne = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                              S = 1),\n                        weights = W_powerlaw(maxlag=5, normalize = TRUE,\n                                             log=TRUE)), # this is new\n              family = \"NegBin1\",\n              subset = subset_fit,\n              # (no specification of par_lag in the control)\n              funct_lag = geometric_lag)\n# now use profile_par_lag (applies a profile likelihood procedure to estimate\n# the lag decay parameter)\nfit7 <- profile_par_lag(noroBE_power, ctrl6)\n\nWarning in profile_par_lag(noroBE_power, ctrl6): Your control list contains a\npar_lag element. This is ignored by profile_par_lag. To fix par_lag rather than\nestimating it from the data use the function hhh4lag.\n\nAIC(fit7)\n\n[1] 17138.12\n\n\nWe can plot the weights as estimated from the data: ::: {.cell hash=‘multivariate_cache/html/plot_weights_09aed0e645717c6254d07816629236cd’}\n# plot the weights assigned to the different lags:\npar(mfrow = 1:2)\nplot(fit7$distr_lag, type = \"h\", xlab  = \"lag\",\n     ylab = \"weight\", ylim = 0:1)\n\n\n\n:::\nAnd consider the Pearson residuals, which look less problematic than before (at least that’s what I like to tell myself). ::: {.cell hash=‘multivariate_cache/html/pr6_3d3b440409e9e73eef7dc10d557aaf87’}\n# check Pearson residuals\npr7 <- pearson_residuals(fit6)\npar(mfrow = c(3, 4))\nfor(unit in colnames(pr7)){\n  acf(pr7[, unit], lag.max = 5, ylim = c(-0.3, 0.3), main = unit)\n}\n\n\n\n:::"
  },
  {
    "objectID": "tutorials/multivariate.html#one-step-ahead-forecasting",
    "href": "tutorials/multivariate.html#one-step-ahead-forecasting",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "One-step-ahead forecasting",
    "text": "One-step-ahead forecasting\nThe packages contain functionality to imitate one-step-ahead (out-of-sample) forecasts retrospectively. To this end, the model is sequentially re-fitted, always including all data up to a given week. Then a one-week-ahead plug-in prediciton is obtained for the next week. We apply this to obtain predictions for weeks 313 through 364 which we had excluded from model fitting so far. Subsequently, you can use scores to evaluate the one-step-ahead predictions using several different statistical metrics. We compute the following:\n\nthe logarithmic score, also called negative predictive log-likelihood (informal explanation: this reflects how likely the observed outcomes were under your predictions)\nthe CRPS (informal explanation: this describes “how far” the observations were from the predictions you issued)\n\nFor both lower values` are better.\n\n##################################################################\n# one-step-ahead forecasting: generate forecasts sequentially\n# compare models 2, 4 and 7\nlog2 <- capture.output(owa2 <- oneStepAhead(fit2, tp = c(312, 363)))\nlog4 <- capture.output(owa4 <- oneStepAhead(fit4, tp = c(312, 363)))\nrm(log2, log4)\n# the weird capture.output formulation is needed to suppress \n# numerous cat() messages.\n# you could also just use\n# owa2 <- oneStepAhead(fit2, tp = c(312, 363))\nowa7 <- oneStepAhead_hhh4lag(fit7, tp = c(312, 363))\n\nLag weights are not re-estimated for the one-step-ahead forecasts (set refit_par_lag = TRUE to re-fit them).\n\n# the return objects contain predictions, observations and a few other things:\nhead(owa7$pred)\n\n         chwi     frkr      lich     mahe      mitt      neuk     pank     rein\n313 10.143891 5.997015  8.315079 7.550797 12.276675  9.072281 18.06990 12.35556\n314 10.052363 6.133326  9.568715 7.025105 12.332588 11.455435 20.52966 10.66938\n315 10.837812 6.638302 10.085745 8.250180 10.817444 10.246092 22.11388 17.05596\n316  9.737658 6.614878 16.126879 8.251785  8.842089  8.346344 24.43109 14.89341\n317  8.017484 5.761749 10.387913 6.531485  6.529512  7.030808 20.62327 11.56332\n318  9.328702 7.435577 13.008535 6.949797  8.054874  9.280143 20.39342 11.20380\n         span     zehl     scho     trko\n313 13.161304 20.62583 19.19221 7.951694\n314 10.103430 23.18901 18.21469 8.461257\n315  8.692397 24.61323 21.20857 8.030405\n316  7.391036 19.24499 17.81345 7.817094\n317 10.879236 19.91117 14.02415 6.814946\n318 11.205468 19.68513 16.13912 8.639782\n\nhead(owa7$observed)\n\n    chwi frkr lich mahe mitt neuk pank rein span zehl scho trko\n313   10    4   10    5   17   16   23    5    7   24   17    5\n314   12    8    9   10    6    8   23   26    3   24   28    3\n315   11    7   31    7    3    5   28   14    4   14   18    2\n316    5    8    6    5    0    5   22    8   18   20   12    4\n317   13   16   20    6    7   11   20    8   14   17   18    6\n318    7    8    7    5    3   16    9   20   12   22   31   11\n\n# plot one-step-ahead point forecasts:\nplot(noroBE@observed[313:364, 1], pch = 20, xlab = \"week\", ylab = \"value\")\nlines(owa2$pred[, 1], type = \"l\", col = 2)\nlines(owa4$pred[, 1], type = \"l\", col = 4)\nlines(owa7$pred[, 1], type = \"l\", col = 6)\n\n\n\n# compute and summarize scores:\ncolMeans(scores(owa2, which = c(\"logs\", \"rps\")))\n\n    logs      rps \n2.506608 2.091024 \n\ncolMeans(scores(owa4, which = c(\"logs\", \"rps\")))\n\n    logs      rps \n2.492431 2.066390 \n\ncolMeans(scores(owa7, which = c(\"logs\", \"rps\")))\n\n    logs      rps \n2.475311 2.042008 \n\n\nWe can see that the more complex model formulations also translate to improved predictive performance (all scores being negatively oriented; see ?scores)."
  },
  {
    "objectID": "tutorials/multivariate.html#predictive-moments-at-longer-forecast-horizons.",
    "href": "tutorials/multivariate.html#predictive-moments-at-longer-forecast-horizons.",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "Predictive moments at longer forecast horizons.",
    "text": "Predictive moments at longer forecast horizons.\nThe hhh4addon package also contains functions to compute predictive moments (means, variances, autocorrelations) for longer time horizons as well as marginal/stationary moments of the fitted model. ::: {.cell hash=‘multivariate_cache/html/moments_67bfcb28ce3d728fc5301eda84d39025’}\n##################################################################\n# longer-term predictive moments can be computed using predictive_moments:\n# predictive moments 10 weeks ahead:\npred_mom7 <- predictive_moments(fit7, t_condition = max(subset_fit), lgt = 10)\n# print some predictive means:\nhead(pred_mom7$mu_matrix[, 1:6])\n\n           chwi     frkr      lich     mahe      mitt     neuk\nt=313 10.143891 5.997015  8.315079 7.550797 12.276675 9.072281\nt=314 10.256036 6.200316  9.202886 7.794306 11.403385 9.483784\nt=315 10.224376 6.285932  9.759873 7.944234 10.889365 9.757613\nt=316 10.077085 6.281429 10.044002 7.998614 10.518741 9.877621\nt=317  9.833192 6.199311 10.132328 7.930412 10.185662 9.836843\nt=318  9.529173 6.051469 10.026521 7.766129  9.843625 9.705029\n\n# plot:\nplot(fit7)\nfanplot_prediction(pred_mom7, add = TRUE,\n                   probs = c(0.05, 0.15, 0.25, 0.75, 0.85, 0.95),\n                   pt.col = \"black\")\n\n\n\n# note: the plot is based on a negative binomial approximation of the \n# predictive distributions.\n\n# stationary/marginal moments are implemented, too (but don't always exist):\nstat7 <- stationary_moments(fit6)\nfanplot_stationary(stat7, unit = 4)\n\n\n\n:::"
  },
  {
    "objectID": "tutorials/multivariate.html#over-to-you",
    "href": "tutorials/multivariate.html#over-to-you",
    "title": "Tutorial, part 2 - Step-by-step development of multivariate hhh4 models",
    "section": "Over to you…",
    "text": "Over to you…\nYou are now invited to perform an analysis of rotavirus in Berlin along the lines of the above. The data, which are structured the same way as the norovirus data. Your “goal” is to formulate a model which will generate good one-step-ahead forecasts (we’ll use logS as the main outcome, which is the so-called logarithmic score, equivalent to the negative predictive log-likelihood). You can load the data via ::: {.cell}\ndata(\"rotaBE\")\n# if you don't have the hhh4addon package installed you can use:\n# load(\n# url(\"https://github.com/cmmid/hhh4-workshop/blob/main/tutorial2_multivariate/data/rotaBE.rda?raw=true\")\n# )\n::: Your code should look somewhat like the following:\n\nLook at the data to get an idea of what it looks like.\nFormulate and refine a control argument. To imitate a forecasting setting it makes sense to only use observations up to week 312 here (though you’re free to play around with whatever subsets of the data you like). ::: {.cell}\n\nctrl <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                               S = 1)),\n              ar = list(f = ~ 0 + fe(1, unitSpecific = TRUE)),\n              family = \"NegBin1\",\n              subset = 6:312)\n::: - Fit the model using hhh4 or profile_par_lag. ::: {.cell}\nyour_fit <- hhh4(rotaBE, ctrl)\n::: - Run the following (remove comments: ::: {.cell}\n# owa <- oneStepAhead(your_fit, tp = c(312, 363))\n# or: owa <- oneStepAhead_hhh4(your_fit, tp = c(312, 363))\n# if you have used profile_par_lag \n# colMeans(scores(owa), which = c(\"logs\", \"rps\"))\n#     logs       rps\n# 1.931189  1.639351\n:::\nIf you like you can post your results here (https://github.com/cmmid/hhh4-workshop/issues/1) or on the chat.\nHere are some model aspects you can play around with:\n\nHigher-order seasonal terms (as in a Fourier series) can be included by setting S = 2 or higher in the addSeason2formula function.\nLinear time trends can be included by adding + t to a formula, e.g. f = addSeason2formula($\\sim$ 0 + t + fe(1, unitSpecific = TRUE), S = 1)\nRotavirus transmission is known to be linked to temperature (https://doi.org/10.1098/rspb.2009.1755). We therefore uploaded a time series of weekly mean temperature values in Berlin (average of daily temperatures at 2pm, lagged by one week) to the GitHub repo of the workshop. These (or a transformation) can be used a a covariate. ::: {.cell hash=‘multivariate_cache/html/load_temp_c14a9cc1c696df6321b52d54b5134389’}\n\n# get temperature data:\ndata_temperature <-\n  read.csv(paste0(\"https://raw.githubusercontent.com/cmmid/hhh4-workshop/\", \n                  \"main/tutorial2_multivariate/data/temperature_berlin.csv\"))\ntemperature <- data_temperature$temperature7d\n# your formula could look as follows:\nctrl <- list(end = list(f = addSeason2formula(~0 + fe(1, unitSpecific = TRUE),\n                                              S = 1)),\n             ar = list(f = ~ 0 + temperature + fe(1, unitSpecific = TRUE)),\n             family = \"NegBin1\",\n             subset = 6:312)\n# though that is not necessarily a very smart way of using the covariate\n:::"
  }
]